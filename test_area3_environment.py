#!/usr/bin/env python3
"""
test_area3_environment.py
ç¬¬3æ–¹é¢é™å®šç’°å¢ƒã®ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import sys
import os
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent
sys.path.append(str(project_root))

from reinforcement_learning.environment.ems_environment import EMSEnvironment
from data_cache import get_emergency_data_cache
import logging

def test_area3_environment():
    """ç¬¬3æ–¹é¢é™å®šç’°å¢ƒã®å‹•ä½œç¢ºèª"""
    print("=" * 80)
    print("ç¬¬3æ–¹é¢é™å®šç’°å¢ƒãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("=" * 80)
    
    # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«è¨­å®š
    logging.basicConfig(level=logging.INFO)
    
    try:
        # 1. ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ãƒ†ã‚¹ãƒˆ
        print("\n1. ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ãƒ†ã‚¹ãƒˆ")
        print("-" * 40)
        cache = get_emergency_data_cache()
        
        # ç¬¬3æ–¹é¢ã®ã‚¨ãƒªã‚¢ãƒ•ã‚£ãƒ«ã‚¿
        area3_districts = ["ç›®é»’åŒº", "æ¸‹è°·åŒº", "ä¸–ç”°è°·åŒº"]
        
        # ãƒ†ã‚¹ãƒˆæœŸé–“ã®ãƒ‡ãƒ¼ã‚¿å–å¾—
        test_period_data = cache.get_period_data(
            "20230401", "20230407", 
            area_filter=area3_districts
        )
        print(f"ç¬¬3æ–¹é¢ã®1é€±é–“ãƒ‡ãƒ¼ã‚¿: {len(test_period_data)}ä»¶")
        
        if len(test_period_data) > 0:
            print("å‡ºå ´å…ˆåŒºå¸‚ã®åˆ†å¸ƒ:")
            if 'å‡ºå ´å…ˆåŒºå¸‚' in test_period_data.columns:
                district_counts = test_period_data['å‡ºå ´å…ˆåŒºå¸‚'].value_counts()
                for district, count in district_counts.items():
                    print(f"  {district}: {count}ä»¶")
            
            print("\nå‚·ç—…åº¦ã®åˆ†å¸ƒ:")
            severity_counts = test_period_data['åå®¹æ‰€è¦‹ç¨‹åº¦'].value_counts()
            for severity, count in severity_counts.head().items():
                print(f"  {severity}: {count}ä»¶")
        
        # 2. EMSç’°å¢ƒã®åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ
        print("\n2. EMSç’°å¢ƒã®åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ")
        print("-" * 40)
        
        config_path = "reinforcement_learning/experiments/config_area3.yaml"
        env = EMSEnvironment(config_path=config_path, mode="train")
        
        print(f"æ•‘æ€¥è»Šæ•°: {env.action_dim}å°")
        print(f"çŠ¶æ…‹ç©ºé–“æ¬¡å…ƒ: {env.state_dim}")
        
        # æ•‘æ€¥è»Šãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
        if hasattr(env, 'ambulance_data') and len(env.ambulance_data) > 0:
            print("\næ•‘æ€¥è»Šåˆ†å¸ƒ:")
            print(f"  ç¬¬3æ–¹é¢ã®æ•‘æ€¥è»Š: {len(env.ambulance_data)}å°")
            
            # sectionã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ç¢ºèª
            if 'section' in env.ambulance_data.columns:
                section_counts = env.ambulance_data['section'].value_counts()
                print("  æ–¹é¢åˆ¥åˆ†å¸ƒ:")
                for section, count in section_counts.items():
                    print(f"    ç¬¬{section}æ–¹é¢: {count}å°")
        
        # 3. ç’°å¢ƒã®ãƒªã‚»ãƒƒãƒˆãƒ†ã‚¹ãƒˆ
        print("\n3. ç’°å¢ƒã®ãƒªã‚»ãƒƒãƒˆãƒ†ã‚¹ãƒˆ")
        print("-" * 40)
        
        initial_obs = env.reset()
        print(f"åˆæœŸè¦³æ¸¬ã®å½¢çŠ¶: {initial_obs.shape}")
        print(f"åˆæœŸè¦³æ¸¬ã®ç¯„å›²: [{initial_obs.min():.3f}, {initial_obs.max():.3f}]")
        
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®äº‹æ¡ˆæ•°ç¢ºèª
        if hasattr(env, 'current_episode_calls'):
            print(f"ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å†…äº‹æ¡ˆæ•°: {len(env.current_episode_calls)}ä»¶")
            
            if len(env.current_episode_calls) > 0:
                first_call = env.current_episode_calls[0]
                print(f"æœ€åˆã®äº‹æ¡ˆ: {first_call.get('severity', 'Unknown')} at {first_call.get('datetime', 'Unknown')}")
        
        # 4. 1ã‚¹ãƒ†ãƒƒãƒ—ã®å®Ÿè¡Œãƒ†ã‚¹ãƒˆ
        print("\n4. 1ã‚¹ãƒ†ãƒƒãƒ—ã®å®Ÿè¡Œãƒ†ã‚¹ãƒˆ")
        print("-" * 40)
        
        # åˆ©ç”¨å¯èƒ½ãªè¡Œå‹•ã‚’ãƒã‚§ãƒƒã‚¯
        action_mask = env.get_action_mask()
        available_actions = [i for i, available in enumerate(action_mask) if available]
        print(f"åˆ©ç”¨å¯èƒ½ãªæ•‘æ€¥è»Š: {len(available_actions)}å°")
        
        if len(available_actions) > 0:
            # æœ€åˆã®åˆ©ç”¨å¯èƒ½ãªæ•‘æ€¥è»Šã‚’é¸æŠ
            test_action = available_actions[0]
            
            # æœ€é©ãªè¡Œå‹•ã‚’å–å¾—
            optimal_action = env.get_optimal_action()
            print(f"ãƒ†ã‚¹ãƒˆè¡Œå‹•: æ•‘æ€¥è»Š{test_action}")
            print(f"æœ€é©è¡Œå‹•: æ•‘æ€¥è»Š{optimal_action}")
            
            # 1ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œ
            result = env.step(test_action)
            if result:
                print(f"å ±é…¬: {result.reward:.3f}")
                print(f"çµ‚äº†ãƒ•ãƒ©ã‚°: {result.done}")
                print(f"æ¬¡ã®è¦³æ¸¬å½¢çŠ¶: {result.observation.shape}")
        
        print("\n5. è¨­å®šå€¤ã®ç¢ºèª")
        print("-" * 40)
        
        config = env.config
        area_restriction = config.get('data', {}).get('area_restriction', {})
        print(f"ã‚¨ãƒªã‚¢åˆ¶é™æœ‰åŠ¹: {area_restriction.get('enabled', False)}")
        print(f"å¯¾è±¡æ–¹é¢: {area_restriction.get('section_code', 'N/A')}")
        print(f"å¯¾è±¡åŒºå¸‚: {', '.join(area_restriction.get('districts', []))}")
        
        ppo_config = config.get('ppo', {})
        print(f"ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {ppo_config.get('n_episodes', 'N/A')}")
        print(f"ãƒãƒƒãƒã‚µã‚¤ã‚º: {ppo_config.get('batch_size', 'N/A')}")
        print(f"å­¦ç¿’ç‡(Actor): {ppo_config.get('learning_rate', {}).get('actor', 'N/A')}")
        
        print("\nâœ… ç¬¬3æ–¹é¢é™å®šç’°å¢ƒã®ãƒ†ã‚¹ãƒˆå®Œäº†")
        return True
        
    except Exception as e:
        print(f"\nâŒ ãƒ†ã‚¹ãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_data_scale_comparison():
    """ãƒ‡ãƒ¼ã‚¿è¦æ¨¡ã®æ¯”è¼ƒ"""
    print("\n" + "=" * 80)
    print("ãƒ‡ãƒ¼ã‚¿è¦æ¨¡æ¯”è¼ƒ")
    print("=" * 80)
    
    cache = get_emergency_data_cache()
    
    # å…¨ä½“ãƒ‡ãƒ¼ã‚¿
    all_data = cache.get_period_data("20230401", "20230407")
    print(f"å…¨23åŒºã®1é€±é–“ãƒ‡ãƒ¼ã‚¿: {len(all_data)}ä»¶")
    
    # ç¬¬3æ–¹é¢ãƒ‡ãƒ¼ã‚¿
    area3_data = cache.get_period_data(
        "20230401", "20230407", 
        area_filter=["ç›®é»’åŒº", "æ¸‹è°·åŒº", "ä¸–ç”°è°·åŒº"]
    )
    print(f"ç¬¬3æ–¹é¢ã®1é€±é–“ãƒ‡ãƒ¼ã‚¿: {len(area3_data)}ä»¶")
    
    if len(all_data) > 0:
        reduction_ratio = len(area3_data) / len(all_data) * 100
        print(f"ãƒ‡ãƒ¼ã‚¿å‰Šæ¸›ç‡: {reduction_ratio:.1f}% (ç´„{100/reduction_ratio:.1f}åˆ†ã®1)")

if __name__ == "__main__":
    success = test_area3_environment()
    test_data_scale_comparison()
    
    if success:
        print("\nğŸ‰ å…¨ã¦ã®ãƒ†ã‚¹ãƒˆãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼")
        print("ç¬¬3æ–¹é¢é™å®šç’°å¢ƒã§å­¦ç¿’ã‚’é–‹å§‹ã§ãã¾ã™ã€‚")
        print("\næ¬¡ã®ã‚³ãƒãƒ³ãƒ‰ã§å­¦ç¿’ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„:")
        print("python train_ppo.py --config reinforcement_learning/experiments/config_area3.yaml")
    else:
        print("\nâš ï¸ ãƒ†ã‚¹ãƒˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
