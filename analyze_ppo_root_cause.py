"""
analyze_ppo_root_cause.py
PPOã®é¸æŠãƒŸã‚¹ã®æ ¹æœ¬åŸå› ã‚’ç‰¹å®š

æ¤œè¨¼é …ç›®:
1. çŠ¶æ…‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®å¦¥å½“æ€§
2. å­¦ç¿’æ™‚ã®è¨­å®šï¼ˆå ±é…¬é–¢æ•°ã€ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ¼ãƒ‰ï¼‰
3. è¡Œå‹•ç¢ºç‡åˆ†å¸ƒã®åˆ†æ
4. å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®åã‚Š
"""

import json
import numpy as np
from pathlib import Path

def analyze_training_config():
    """å­¦ç¿’æ™‚ã®è¨­å®šã‚’åˆ†æ"""
    
    print("=" * 80)
    print("Phase 1: å­¦ç¿’æ™‚ã®è¨­å®šåˆ†æ")
    print("=" * 80)
    
    config_path = Path('reinforcement_learning/experiments/ppo_training/ppo_20251017_113908/configs/config.json')
    
    if not config_path.exists():
        print(f"âŒ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {config_path}")
        return None
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = json.load(f)
    
    print("\nã€é‡è¦ãªè¨­å®šé …ç›®ã€‘")
    print("-" * 80)
    
    # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ¼ãƒ‰
    hybrid_mode = config.get('hybrid_mode', False)
    print(f"\n1. ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ¼ãƒ‰: {hybrid_mode}")
    if hybrid_mode:
        severe_conditions = config.get('severe_conditions', [])
        print(f"   é‡ç—‡ç³»ï¼ˆç›´è¿‘éšŠï¼‰: {severe_conditions}")
        print(f"   âš ï¸ å­¦ç¿’æ™‚ã«ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ¼ãƒ‰ãŒæœ‰åŠ¹")
        print(f"   â†’ è»½ç—‡ç³»ã®ã¿ã§å­¦ç¿’ â†’ ãƒ‡ãƒ¼ã‚¿ãŒåã£ã¦ã„ã‚‹å¯èƒ½æ€§")
    
    # å ±é…¬é–¢æ•°
    reward_mode = config.get('reward_mode', 'unknown')
    print(f"\n2. å ±é…¬ãƒ¢ãƒ¼ãƒ‰: {reward_mode}")
    
    reward_config = config.get('reward_config', {})
    if reward_config:
        print(f"   å ±é…¬è¨­å®š:")
        for key, value in reward_config.items():
            print(f"     {key}: {value}")
        
        # é‡è¦ãªé‡ã¿
        if 'response_time_weight' in reward_config:
            rt_weight = reward_config['response_time_weight']
            coverage_weight = reward_config.get('coverage_weight', 0)
            print(f"\n   âš ï¸ é‡ã¿æ¯”è¼ƒ:")
            print(f"     å¿œç­”æ™‚é–“: {rt_weight}")
            print(f"     ã‚«ãƒãƒ¬ãƒƒã‚¸: {coverage_weight}")
            
            if coverage_weight > rt_weight:
                print(f"   ğŸ”¥ å•é¡Œç™ºè¦‹: ã‚«ãƒãƒ¬ãƒƒã‚¸é‡è¦– > å¿œç­”æ™‚é–“")
                print(f"   â†’ PPOãŒé ã„éšŠã‚’é¸æŠã—ã¦ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚’ç¶­æŒã™ã‚‹å¯èƒ½æ€§")
    
    # çŠ¶æ…‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    state_config = config.get('state_config', {})
    print(f"\n3. çŠ¶æ…‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°:")
    print(f"   è¨­å®š: {state_config}")
    
    # å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    training_config = config.get('training', {})
    print(f"\n4. å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
    print(f"   ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {training_config.get('num_episodes', 'N/A')}")
    print(f"   ãƒãƒƒãƒã‚µã‚¤ã‚º: {training_config.get('batch_size', 'N/A')}")
    
    return config

def analyze_reward_design():
    """å ±é…¬è¨­è¨ˆã‚’è©³ç´°åˆ†æ"""
    
    print("\n" + "=" * 80)
    print("Phase 2: å ±é…¬è¨­è¨ˆã®è©³ç´°åˆ†æ")
    print("=" * 80)
    
    # reward_designer.pyã‚’èª­ã¿è¾¼ã‚“ã§ç¢ºèª
    from reinforcement_learning.environment.reward_designer import RewardDesigner
    
    print("\nRewardDesignerã®å®Ÿè£…ã‚’ç¢ºèªä¸­...")
    print("ï¼ˆã‚³ãƒ¼ãƒ‰å†…å®¹ã‚’æ‰‹å‹•ã§ç¢ºèªã—ã¦ãã ã•ã„ï¼‰")
    print()
    print("ç¢ºèªãƒã‚¤ãƒ³ãƒˆ:")
    print("  1. _calculate_hybrid_reward ã®å®Ÿè£…")
    print("  2. å¿œç­”æ™‚é–“å ±é…¬ã®è¨ˆç®—å¼")
    print("  3. ã‚«ãƒãƒ¬ãƒƒã‚¸å ±é…¬ã®é‡ã¿")
    print("  4. ãƒšãƒŠãƒ«ãƒ†ã‚£ã®å¤§ãã•")

def check_state_encoding():
    """çŠ¶æ…‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®å¦¥å½“æ€§ç¢ºèª"""
    
    print("\n" + "=" * 80)
    print("Phase 3: çŠ¶æ…‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®ç¢ºèª")
    print("=" * 80)
    
    print("\nç¢ºèªãƒã‚¤ãƒ³ãƒˆ:")
    print("  1. äº‹æ¡ˆä½ç½®ï¼ˆh3_indexï¼‰ãŒæ­£ã—ãã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹ã‹")
    print("  2. æ•‘æ€¥è»Šä½ç½®ãŒæ­£ã—ãã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹ã‹")
    print("  3. è·é›¢æƒ…å ±ãŒé©åˆ‡ã«è¡¨ç¾ã•ã‚Œã¦ã„ã‚‹ã‹")
    print("  4. æ­£è¦åŒ–ãŒé©åˆ‡ã‹")
    
    print("\nâ†’ state_encoder.py ã¨ modular_state_encoder.py ã‚’ç¢ºèª")

def propose_improvements():
    """æ”¹å–„æ¡ˆã®æç¤º"""
    
    print("\n" + "=" * 80)
    print("æ”¹å–„æ¡ˆ")
    print("=" * 80)
    
    print("\nã€çŸ­æœŸçš„æ”¹å–„æ¡ˆï¼ˆå³åº§ã«å®Ÿè¡Œå¯èƒ½ï¼‰ã€‘")
    print()
    print("1. ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ¼ãƒ‰ã‚’ç„¡åŠ¹ã«ã—ã¦å†å­¦ç¿’")
    print("   - å…¨äº‹æ¡ˆã§å­¦ç¿’ â†’ ãƒ‡ãƒ¼ã‚¿ã®åã‚Šã‚’è§£æ¶ˆ")
    print("   - æ¨è«–æ™‚ã®ã¿ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ç”¨")
    print()
    print("2. å ±é…¬é–¢æ•°ã®é‡ã¿èª¿æ•´")
    print("   - å¿œç­”æ™‚é–“ã®é‡ã¿ã‚’å¢—åŠ ï¼ˆä¾‹: 0.4 â†’ 0.7ï¼‰")
    print("   - ã‚«ãƒãƒ¬ãƒƒã‚¸ã®é‡ã¿ã‚’æ¸›å°‘ï¼ˆä¾‹: 0.5 â†’ 0.2ï¼‰")
    print()
    print("3. ç°¡æ˜“ä¿®æ­£: PPOã‚’ç›´è¿‘éšŠã«ç½®ãæ›ãˆ")
    print("   - æš«å®šçš„ã«å…¨äº‹æ¡ˆã§ç›´è¿‘éšŠé‹ç”¨")
    print("   - æ€§èƒ½ã¯ä¿è¨¼ã•ã‚Œã‚‹ãŒã€å­¦ç¿’ã®æ„å‘³ãŒãªããªã‚‹")
    
    print("\nã€ä¸­æœŸçš„æ”¹å–„æ¡ˆï¼ˆå†å­¦ç¿’ãŒå¿…è¦ï¼‰ã€‘")
    print()
    print("4. çŠ¶æ…‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®æ”¹å–„")
    print("   - è·é›¢æƒ…å ±ã‚’æ˜ç¤ºçš„ã«è¿½åŠ ")
    print("   - ã‚°ãƒªãƒƒãƒ‰è¡¨ç¾ã‚’æ”¹å–„")
    print()
    print("5. å ±é…¬é–¢æ•°ã®å†è¨­è¨ˆ")
    print("   - å¿œç­”æ™‚é–“ã‚’ä¸»è¦å ±é…¬ã«")
    print("   - ã‚«ãƒãƒ¬ãƒƒã‚¸ã¯è£œåŠ©çš„å ±é…¬ã«")
    
    print("\nã€é•·æœŸçš„æ”¹å–„æ¡ˆï¼ˆæ ¹æœ¬çš„è¦‹ç›´ã—ï¼‰ã€‘")
    print()
    print("6. æ¨¡å€£å­¦ç¿’ï¼ˆImitation Learningï¼‰ã®å°å…¥")
    print("   - ç›´è¿‘éšŠæˆ¦ç•¥ã®è¡Œå‹•ã‚’æ•™å¸«ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦å­¦ç¿’")
    print("   - ãã®å¾Œã€å¼·åŒ–å­¦ç¿’ã§å¾®èª¿æ•´")
    print()
    print("7. ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’")
    print("   - è¤‡æ•°ã®ç›®çš„ï¼ˆå¿œç­”æ™‚é–“ã€ã‚«ãƒãƒ¬ãƒƒã‚¸ã€å…¬å¹³æ€§ï¼‰ã‚’åŒæ™‚æœ€é©åŒ–")

def main():
    """ãƒ¡ã‚¤ãƒ³åˆ†æ"""
    
    print("=" * 80)
    print("PPOæ ¹æœ¬åŸå› åˆ†æãƒ„ãƒ¼ãƒ«")
    print("=" * 80)
    
    # Phase 1: å­¦ç¿’è¨­å®š
    config = analyze_training_config()
    
    # Phase 2: å ±é…¬è¨­è¨ˆ
    analyze_reward_design()
    
    # Phase 3: çŠ¶æ…‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    check_state_encoding()
    
    # æ”¹å–„æ¡ˆ
    propose_improvements()
    
    print("\n" + "=" * 80)
    print("åˆ†æå®Œäº†")
    print("=" * 80)
    
    print("\næ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print("  1. reward_designer.py ã§å ±é…¬é–¢æ•°ã®é‡ã¿ã‚’ç¢ºèª")
    print("  2. å­¦ç¿’æ™‚ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ¼ãƒ‰è¨­å®šã‚’ç¢ºèª")
    print("  3. æ”¹å–„æ¡ˆã‹ã‚‰1ã¤é¸æŠã—ã¦å®Ÿè£…")

if __name__ == "__main__":
    main()

