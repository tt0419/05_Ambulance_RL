# 緊急診断ガイド - 学習とテストの性能乖離問題

## 🚨 状況のまとめ

### 観測された事実

**学習時**（ppo_20251110_135103、直近100エピソード）:
- 日付: 2023-06-15
- 設定: 100%教師あり（teacher_prob = 1.0）、hybridなし
- 結果: 平均RT **7.46分**、重症6分達成率 **31.0%**

**テスト時**（同じ日付2023-06-15で100回実行）:
- 日付: 2023-06-15（学習と同一）
- モデル: `ppo_20251110_182404/checkpoints/best_model.pth`
- 結果: 平均RT **20.88分**、重症6分達成率 **1.587%**

### 🔥 決定的な矛盾

**過学習を前提に同じデータで100%教師あり学習しているのに、テスト性能が全く異なる**

→ これは、**学習時のログに表示されている7.46分は「教師の行動を環境が実行した結果」であり、PPOエージェント自体は全く学習していない**ことを強く示唆します。

---

## 🔍 診断の実行手順

以下の2つのスクリプトを順番に実行してください。

### ステップ1: 学習プロセスの診断

**目的**: PPO学習が実際に機能しているかを確認

```bash
python debug_training_process.py --config config.yaml --episodes 5
```

**このスクリプトが検証すること**:

1. ✅ **バッファへのデータ蓄積**
   - `store_transition()`が正しく呼ばれているか
   - バッファサイズが十分か（バッチサイズ以上）

2. ✅ **PPOの更新実行**
   - `agent.update()`が実際に実行されるか
   - actor_loss, critic_lossが計算されるか

3. ✅ **モデルパラメータの変化**（最重要）
   - 学習前後でパラメータが変化するか
   - ActorとCriticの両方が更新されるか

4. ✅ **モデルの保存・読み込み**
   - 保存したモデルが正しく読み込めるか

**期待される結果**:

```
✅ 【検証1】バッファへのデータ蓄積
✅ 【検証2】PPOの更新実行
✅ 【検証3】モデルパラメータの変化  ← これが最重要
✅ 【検証4】モデルの保存・読み込み
```

**もし検証3が❌の場合**:
```
❌ 【検証3】モデルパラメータの変化
   ActorもCriticもパラメータが変化していません！
   → 学習が全く機能していません
```
→ これが現在の問題の根本原因です

---

### ステップ2: テスト時の行動選択診断

**目的**: 学習済みモデルが正しく使用されているかを確認

```bash
python debug_test_behavior.py \
    --model reinforcement_learning/experiments/ppo_training/ppo_20251110_182404/checkpoints/best_model.pth \
    --config config.yaml \
    --steps 50
```

**このスクリプトが検証すること**:

1. **モデルパラメータのチェック**
   - パラメータが初期化されているか
   - ゼロや極端な値でないか

2. **行動選択の分析**
   - エージェントの選択と最適行動の一致率
   - 選択された行動の確率分布
   - 最適行動の確率

3. **ランダム選択との比較**
   - 学習済みエージェント vs ランダム選択
   - 報酬の差分

**期待される結果（学習が成功している場合）**:
```
✅ モデルパラメータが初期化されている
✅ ランダム選択より高性能
✅ 教師一致率が50%以上 (85.2%)
```

**期待される結果（学習が失敗している場合）**:
```
✅ モデルパラメータが初期化されている
❌ ランダム選択より高性能
❌ 教師一致率が50%以上 (8.3%)
教師一致率: 8.3% (5/60)
→ エージェントはランダムに近い行動選択をしています
→ 学習が全く機能していない可能性が高いです
```

---

## 📊 診断結果に基づく対応

### ケース1: 検証3（パラメータ変化）が❌

**症状**:
```
❌ 【検証3】モデルパラメータの変化
   ActorもCriticもパラメータが変化していません！
```

**原因の可能性**:

1. **`agent.update()`が呼ばれていない**
   - バッファサイズ不足
   - trainer.pyの実装ミス

2. **勾配が計算されていない**
   - `torch.no_grad()`の範囲が広すぎる
   - `.backward()`が呼ばれていない

3. **学習率が0または極小**
   - オプティマイザの設定ミス

**確認方法**:

```python
# trainer.pyのL133-135付近を確認
if len(self.agent.buffer) >= self.agent.batch_size:
    update_stats = self.agent.update()  # ← これが実際に呼ばれているか
    self.training_stats.append(update_stats)
```

**ログに以下が出力されているか確認**:
- バッファサイズ
- update()の実行回数
- actor_loss, critic_lossの値

**対策**:

1. バッファサイズの確認
   ```python
   print(f"バッファサイズ: {len(self.agent.buffer)}/{self.agent.batch_size}")
   ```

2. update()の実行を確認
   ```python
   if len(self.agent.buffer) >= self.agent.batch_size:
       print("update()を実行します")  # デバッグログ
       update_stats = self.agent.update()
       print(f"actor_loss: {update_stats['actor_loss']}")
   ```

3. 学習率の確認
   ```yaml
   # config.yamlのppo.learning_rateセクション
   learning_rate:
     actor: 0.0003  # ← 0でないか確認
     critic: 0.001  # ← 0でないか確認
   ```

---

### ケース2: 検証1（バッファ蓄積）が❌

**症状**:
```
❌ 【検証1】バッファへのデータ蓄積
   バッファにデータが蓄積されていません！
```

**原因**:
- `store_transition()`が呼ばれていない
- バッファの実装に問題がある

**対策**:

trainer.pyのL331-345付近を確認:
```python
if training:
    self.agent.store_transition(  # ← これが呼ばれているか
        state=state,
        action=action,
        reward=reward,
        next_state=next_state,
        done=done,
        log_prob=log_prob,
        value=value,
        action_mask=action_mask
    )
```

---

### ケース3: テスト時の教師一致率が極端に低い（<10%）

**症状**:
```
教師一致率: 8.3% (5/60)
❌ エージェントはランダムに近い行動選択をしています
```

**原因**:
- 学習が全く機能していない
- ステップ1の検証3が失敗している可能性が高い

**対策**:
1. ステップ1の診断を実行
2. パラメータ変化を確認
3. 学習ログを詳細に確認

---

### ケース4: すべての検証が✅だが、テスト性能が悪い

**症状**:
- 学習プロセスは正常に見える
- モデルパラメータも変化している
- しかしテスト性能が20分台

**原因の可能性**:

1. **学習時のログが誤解を招いている**
   - 表示されている7.46分は教師の性能
   - エージェント自体の性能ではない

2. **バッファサイズが小さすぎる**
   - エピソード長に対してバッチサイズが大きすぎる
   - 1エピソードで十分なデータが集まらない

3. **教師確率の設定ミス**
   - 100%教師ありのはずが、実際は異なる確率

**対策**:

1. 学習時のログを詳細に確認
   ```python
   # trainer.pyに以下を追加
   print(f"エピソード{episode}: バッファサイズ={len(self.agent.buffer)}")
   if len(self.agent.buffer) >= self.agent.batch_size:
       print("  → PPO更新を実行")
       update_stats = self.agent.update()
       print(f"  actor_loss={update_stats['actor_loss']:.4f}")
   else:
       print("  → バッファ不足、更新なし")
   ```

2. エピソード統計の確認
   ```python
   # 教師一致率の計算を追加
   matched_count = 0
   total_steps = 0
   # ... (エピソード実行中に集計)
   print(f"教師一致率: {matched_count/total_steps:.1%}")
   ```

---

## 🎯 最も可能性が高いシナリオ

現在の症状から判断すると、以下のシナリオが最も可能性が高いです：

### シナリオ: 学習が全く機能していない

**症状**:
1. 学習時: 7.46分（良好に見える）
2. テスト時: 20.88分（悪い）
3. 同じデータで過学習を前提としているのに大きな乖離

**原因**:
- PPOの`update()`が実際には呼ばれていない、または
- パラメータが更新されていない

**根拠**:
1. バッファサイズ不足でupdate()がスキップされている
2. 学習率が0または極小値
3. 勾配計算に問題がある

**学習時のログの解釈**:
- 表示されている7.46分は**教師の行動を環境が実行した結果**
- PPOエージェント自体は未学習のまま
- テスト時にエージェントを使うため20分台になる

**検証方法**:
```bash
# ステップ1を実行
python debug_training_process.py --config config.yaml --episodes 5
```

もし検証3が❌なら、これが決定的な証拠です。

---

## 📝 診断実行後の報告フォーマット

診断を実行したら、以下の情報を報告してください：

### 1. ステップ1の結果

```
【検証1】バッファへのデータ蓄積: ✅ or ❌
【検証2】PPOの更新実行: ✅ or ❌
【検証3】モデルパラメータの変化: ✅ or ❌  ← 最重要
【検証4】モデルの保存・読み込み: ✅ or ❌

パラメータ変化の詳細:
  Actor層0: パラメータ変化量 = X.XXXXXX
  Critic層0: パラメータ変化量 = X.XXXXXX
```

### 2. ステップ2の結果

```
教師一致率: XX.X% (XX/XX)
学習済みエージェント総報酬: XX.XX
ランダム選択総報酬: XX.XX
差分: ±XX.XX
```

### 3. 学習ログ（可能であれば）

```
エピソード1: バッファサイズ=XXX
  → PPO更新を実行 (or → バッファ不足、更新なし)
  actor_loss=X.XXXX
```

---

## 🚀 次のアクション

1. **即座に実行**: 
   ```bash
   python debug_training_process.py --config config.yaml --episodes 5
   ```

2. **結果を報告**: 上記のフォーマットで結果を共有

3. **根本原因の特定**: 診断結果に基づいて対策を決定

**検証3が❌の場合、それが決定的な証拠となります。**
