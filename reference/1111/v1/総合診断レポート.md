# 学習とテストの性能乖離問題 - 総合診断レポート

**作成日**: 2025年11月11日  
**問題**: 学習時は7.46分の性能を示すが、テスト時は20.88分に悪化

---

## 🎯 問題の本質

### 観測された決定的な矛盾

**学習時の結果**（ppo_20251110_135103）:
- 日付: 2023-06-15
- 設定: 100%教師あり学習（teacher_prob = 1.0）
- 結果: **平均RT 7.46分**、重症6分達成率 **31.0%**

**テスト時の結果**（同じ日付で100回実行）:
- 日付: 2023-06-15（学習と同一）
- モデル: best_model.pth
- 結果: **平均RT 20.88分**、重症6分達成率 **1.587%**

### 💡 この矛盾が意味すること

**過学習を前提に同じデータで学習しているにもかかわらず、3倍近い性能差**

これは以下のことを強く示唆します：

1. **学習時のログに表示されている7.46分は「教師の行動を環境が実行した結果」**
2. **PPOエージェント自体は全く学習していない**
3. **テスト時にはエージェントの未学習状態が露呈し、20分台になる**

---

## 🔍 推定される根本原因

### 最も可能性が高い原因: バッファサイズ不足

**プロジェクト設定を確認したところ**:
- `batch_size: 1024`（config_tokyo23.yamlより）
- エピソード長: 24時間シミュレーション

**問題の流れ**:

1. **エピソード実行中**: 教師の行動で環境を実行 → 7.46分の結果
2. **バッファへの蓄積**: `store_transition()`でデータを蓄積
3. **更新の試み**: `if len(self.agent.buffer) >= self.agent.batch_size:`
4. **更新がスキップされる**: バッファサイズが1024未満のため条件を満たさない
5. **パラメータ未更新**: モデルはランダム初期化のまま
6. **テスト時**: 未学習のエージェントを使用 → 20分台

**trainer.pyの該当コード**（L133-135）:
```python
# PPO更新
if len(self.agent.buffer) >= self.agent.batch_size:  # ← この条件を満たさない
    update_stats = self.agent.update()
    self.training_stats.append(update_stats)
```

**エピソード長の計算**:
- 24時間シミュレーション
- time_per_step = 60秒（推定）
- 最大ステップ数 = 24 * 60 = 1440ステップ

しかし、実際には：
- `pending_call`がない場合はステップがスキップされる
- エピソード終了条件で早期に終了する可能性
- 実際のステップ数は数百程度？

**結論**: 1エピソードでバッファサイズ1024に到達していない可能性が高い

---

## 📊 検証すべき仮説

### 仮説1: バッファサイズが不足している（可能性：高）

**検証方法**:
```bash
python debug_training_process.py --config config.yaml --episodes 5
```

**期待される結果**:
```
バッファサイズ: 456/1024  ← 1024未満
⚠️ 【警告】バッファサイズがバッチサイズ未満です
    → update()は実行されません
❌ 【検証3】モデルパラメータの変化
   ActorもCriticもパラメータが変化していません！
```

**対策**:
1. `batch_size`を小さくする（例: 256または512）
2. 複数エピソード蓄積してから更新する
3. エピソード長を長くする

### 仮説2: update()が呼ばれているが、勾配が計算されていない（可能性：低）

**検証方法**: 同上のデバッグスクリプト

**期待される結果**:
```
✅ 【検証2】PPOの更新実行
   actor_loss: 0.XXXX
❌ 【検証3】モデルパラメータの変化
   パラメータ変化量 = 0.000000
```

**対策**: PPOの実装を詳細に確認

### 仮説3: 学習率が0または極小（可能性：低）

**確認方法**: config.yamlを確認

```yaml
learning_rate:
  actor: 0.0003  # ← これが0でないか
  critic: 0.001
```

---

## 🎯 診断の実行手順（重要）

### ステップ1: 学習プロセス診断

```bash
python debug_training_process.py --config config.yaml --episodes 5
```

**このスクリプトが明らかにすること**:
1. バッファにデータが蓄積されているか
2. バッファサイズがbatch_size以上になるか
3. update()が実際に実行されるか
4. **パラメータが変化するか（最重要）**

### ステップ2: テスト時の診断

```bash
python debug_test_behavior.py \
    --model reinforcement_learning/experiments/ppo_training/ppo_20251110_182404/checkpoints/best_model.pth \
    --config config.yaml \
    --steps 50
```

**このスクリプトが明らかにすること**:
1. モデルが正しく読み込まれているか
2. エージェントの行動選択が教師と一致するか
3. ランダム選択と比較して性能が高いか

---

## 📋 期待される診断結果と対応

### パターンA: バッファサイズ不足（最も可能性が高い）

**診断結果**:
```
バッファサイズ: XXX/1024  ← 1024未満
⚠️ バッファサイズがバッチサイズ未満です
❌ 【検証2】PPOの更新実行
   バッファサイズが不足しているため、update()は実行されません
❌ 【検証3】モデルパラメータの変化
   ActorもCriticもパラメータが変化していません！
```

**対策**:

#### 対策1: batch_sizeを小さくする（推奨）

config.yamlを修正:
```yaml
ppo:
  batch_size: 256  # 1024 → 256に変更
```

#### 対策2: 複数エピソード蓄積してから更新

trainer.pyを修正:
```python
# L133-135を以下に変更
# 10エピソードごとに更新、またはバッファサイズが2048以上で更新
if episode % 10 == 0 or len(self.agent.buffer) >= 2048:
    if len(self.agent.buffer) >= self.agent.batch_size:
        update_stats = self.agent.update()
        self.training_stats.append(update_stats)
```

#### 対策3: バッファをクリアしないで蓄積

ppo_agent.pyのupdate()メソッドを修正:
```python
# L360-361をコメントアウト
# self.buffer.clear()  # ← これをコメントアウト
```

ただし、これはPPOの本来の動作ではないため推奨しません。

---

### パターンB: 学習は機能しているが、学習時のログが誤解を招く

**診断結果**:
```
✅ 【検証1】バッファへのデータ蓄積
✅ 【検証2】PPOの更新実行
✅ 【検証3】モデルパラメータの変化
✅ 【検証4】モデルの保存・読み込み
```

**しかしテスト性能は悪い**

**原因**: 
- 学習時のログは教師の行動による環境の結果（7.46分）を表示
- エージェント自体の性能は別途測定が必要
- 100%教師ありでも、学習には時間がかかる

**対策**:
1. より多くのエピソードで学習（10,000エピソード以上）
2. 学習曲線を確認（wandbなど）
3. 教師確率を徐々に減らす設定を確認

---

### パターンC: その他の問題

**診断結果**:
```
✅ 【検証1】バッファへのデータ蓄積
✅ 【検証2】PPOの更新実行
❌ 【検証3】モデルパラメータの変化
   Actor: パラメータ変化量 = 0.000001 (極小)
```

**原因**: 学習率が小さすぎる、または勾配計算に問題

**対策**:
1. 学習率を上げる（actor: 0.001, critic: 0.003など）
2. 勾配クリッピングの閾値を確認
3. PPOの実装を詳細にレビュー

---

## 🚀 即座に実行すべきこと

### 1. 診断スクリプトの実行

```bash
# 最優先で実行
python debug_training_process.py --config config.yaml --episodes 5
```

### 2. 結果の報告

以下の情報を報告してください：

```
【検証1】バッファへのデータ蓄積: ✅ or ❌
  - 最終バッファサイズ: XXX
  - バッチサイズ: 1024

【検証2】PPOの更新実行: ✅ or ❌
  - actor_loss: X.XXXX
  - critic_loss: X.XXXX

【検証3】モデルパラメータの変化: ✅ or ❌
  - Actor層0: パラメータ変化量 = X.XXXXXX
  - Critic層0: パラメータ変化量 = X.XXXXXX

その他の情報:
  - エピソード長（平均ステップ数）: XXX
  - 1エピソードでバッファに追加されるデータ数: XXX
```

### 3. 暫定的な対策（診断前でも実施可能）

**最も可能性が高いのはバッファサイズ不足なので**:

config.yamlを修正:
```yaml
ppo:
  batch_size: 256  # 1024 → 256に変更
  n_episodes: 500  # テスト用に少なくする
```

再学習を実行:
```bash
python train_ppo.py --config config.yaml
```

学習時のログで以下を確認:
```
エピソードXXX: バッファサイズ=XXX
  → PPO更新を実行  ← これが表示されるか
  actor_loss=X.XXXX
```

---

## 📝 まとめ

### 問題の本質
- 学習時の7.46分は**教師の性能**であり、エージェントの性能ではない
- PPOエージェントは**全く学習していない**可能性が高い
- テスト時に未学習状態が露呈し、20分台になる

### 最も可能性が高い原因
- **バッファサイズ（1024）に対してエピソード長が短い**
- update()が実行されず、パラメータが更新されない

### 即座の対策
1. 診断スクリプトで原因を確定
2. `batch_size`を256に下げる
3. 再学習して改善を確認

### 長期的な対策
1. エピソード長の最適化
2. バッファ蓄積戦略の見直し
3. 学習曲線のモニタリング強化

**診断スクリプトの実行結果を待っています。**
