# PPOエージェント学習・テスト性能問題 - 原因分析と修正案検証レポート

**作成日**: 2025年11月11日  
**分析対象**: trainer.py, ppo_agent.py  
**問題**: 学習時は良好な成績だが、テスト環境で性能が発揮されない

---

## 📊 問題の全体像

### ユーザーが報告している現象
- **学習時**: 良い成績を達成（例: 平均応答時間 7.46分）
- **テスト時**: ランダム初期化と変わらない性能（例: 平均応答時間 20分台）
- **結論**: 学習結果が正しく保持・適用されていない

---

## 🔍 コードレビュー結果

### 1. trainer.pyの現状（L257-274）

```python
# 現在の実装
if training:
    action, log_prob, value = self.agent.select_action_with_teacher(
        state,
        action_mask,
        optimal_action,
        teacher_prob,
        deterministic=False
    )
    matched_teacher = (action == optimal_action) if optimal_action is not None else False
else:
    action, log_prob, value = self.agent.select_action_with_teacher(
        state,
        action_mask,
        optimal_action,
        1.0 if force_teacher else 0.0,
        deterministic=True
    )
    matched_teacher = (action == optimal_action) if optimal_action is not None else False
```

**✅ 結論**: この部分は既に修正済みです。文書で指摘されていた危険な直接的log_prob計算は存在しません。

### 2. ppo_agent.pyの現状（L180-226）

```python
def select_action_with_teacher(self, 
                            state: np.ndarray,
                            action_mask: Optional[np.ndarray],
                            optimal_action: Optional[int],
                            teacher_prob: float = 0.5,
                            deterministic: bool = False) -> Tuple[int, float, float]:
    # 教師の行動が利用可能で、確率的に教師を選択
    if optimal_action is not None and np.random.random() < teacher_prob:
        # 教師の行動を選択
        action = optimal_action
        
        # その行動の対数確率と価値を計算
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            # Actor出力
            action_probs = self.actor(state_tensor)
            
            # 選択された行動の確率 - ★数値安定化あり★
            if action < action_probs.shape[1]:
                log_prob = torch.log(action_probs[0, action] + 1e-8).item()  # ← ★安全★
            else:
                log_prob = -10.0  # 範囲外の場合のペナルティ
            
            # Critic出力
            value = self.critic(state_tensor).squeeze().item()
        
        return action, log_prob, value
    else:
        # 通常のPPO選択
        return self.select_action(state, action_mask, deterministic)
```

**✅ 結論**: L216で`+ 1e-8`による数値安定化が実装されています。文書で指摘されていたlog(0)の問題は回避されています。

### 3. advance_time()の実装状況（L213-214）

```python
# ★★★ 時間を進めてイベント処理（教師あり学習のため） ★★★
self.env.advance_time()

# 行動選択
action_mask = self.env.get_action_mask()
...
optimal_action = self.env.get_optimal_action() if teacher_prob > 0 else None
```

**✅ 結論**: `advance_time()`が正しく呼び出されています。`optimal_action`取得のタイミング問題も修正済みです。

---

## 🎯 修正案の評価

### 提供された修正案の内容

文書では以下の修正を提案していました：

```python
# 修正案
action, log_prob, value = self.agent.select_action_with_teacher(
    state,
    action_mask,
    optimal_action,
    teacher_prob if training else (1.0 if force_teacher else 0.0),
    deterministic=not training
)
```

### ✅ 妥当性の評価

**結論: 修正案は正しいが、既に実装済み**

現在のコード（L257-274）は、修正案とほぼ同等の実装になっています。唯一の違いは、`training`と`else`で分岐している点ですが、機能的には同じです。

---

## 🚨 **真の問題：なぜテスト時に性能が出ないのか？**

コードレビューの結果、提供されたtrainer.pyとppo_agent.pyには文書で指摘されていたバグは**存在しません**。では、なぜテスト時に性能が発揮されないのでしょうか？

### 仮説1: 修正前のコードで学習したモデルを使用している

**可能性: 高**

- 修正前のコードで学習したモデル（.pthファイル）は、正しく学習されていない
- そのモデルをテスト時にロードしても、ランダム初期化と変わらない性能になる
- **対策**: 修正後のコードで再学習が必要

### 仮説2: optimal_actionが正しく取得できていない

**可能性: 中**

trainer.pyのデバッグログ（L231-256）を見ると、以下の警告があります：

```python
if optimal_action is None and available_count > 0:
    print(f"  [WARNING] 利用可能な救急車があるのにoptimal_actionがNone!")
```

これは、`advance_time()`が実装されていても、`env.get_optimal_action()`が正しく動作していない可能性を示唆しています。

**確認方法**:
```python
# 学習時のログを確認
# optimal_actionがNoneになっているケースの頻度を確認
```

### 仮説3: テスト時の評価設定に問題がある

**可能性: 中**

評価メソッド（_evaluate）を確認したところ、L413-417に以下のコードがあります：

```python
# 今回の「完全模倣実験」では、評価時も教師を強制的に有効にする
force_teacher_for_eval = self.config.get('teacher', {}).get('final_prob', 0) == 1.0

episode_reward, _, episode_stats = self._run_episode(
    training=False, 
    force_teacher=force_teacher_for_eval
)
```

これは、`final_prob`が1.0の場合のみ教師を使用する設定になっています。しかし、**テスト時にはPPOエージェントの学習結果を評価すべき**なので、`force_teacher=False`にすべきです。

**問題**: テスト時に教師を使う設定になっている可能性

### 仮説4: 学習データとテストデータの分布シフト

**可能性: 低**

- 学習データとテストデータで環境の特性が大きく異なる場合、過学習が発生する
- しかし、これは通常「ある程度の性能劣化」であり、「ランダム初期化レベル」にまで落ちることは稀

---

## 📋 推奨される対応手順

### ステップ1: 現在のコードで学習を実行

```bash
# 修正済みコードで新たに学習を実行
python train_ppo.py --config config.yaml
```

**確認ポイント**:
1. 学習時のログで`optimal_action`が正しく取得できているか
2. 教師一致率（matched_teacher率）が高いか（80%以上を期待）
3. 学習が収束しているか

### ステップ2: 学習時のログを詳細に確認

特に以下を確認：

```
[TEACHER DEBUG] Episode開始
  teacher_prob: 0.8992  ← 教師確率が適切か
  
[ACTION DEBUG] Step 0
  optimal_action: 159  ← Noneでないか
  use_teacher: True    ← 教師が使われているか
```

### ステップ3: テスト時の評価方法を修正

**問題**: 現在のコードでは評価時に条件付きで教師を使用している

**修正案**:

```python
# trainer.py L410-418を以下に修正

for _ in range(self.n_eval_episodes):
    # ★★★ 評価時は学習済みPPOエージェントの性能を測定する ★★★
    # force_teacher=False で、エージェントの学習結果を評価
    episode_reward, _, episode_stats = self._run_episode(
        training=False, 
        force_teacher=False  # ← 常にFalse
    )
    eval_rewards.append(episode_reward)
```

### ステップ4: モデルの保存・読み込みを確認

```python
# モデル保存（trainer.pyで既に実装済み）
self.agent.save(path)

# モデル読み込み時の確認
checkpoint = torch.load(path, map_location=self.device)
self.actor.load_state_dict(checkpoint['actor_state_dict'])
self.critic.load_state_dict(checkpoint['critic_state_dict'])

# ★ 読み込み後、ネットワークが学習済みパラメータを持っているか確認
print(f"Actor パラメータ例: {list(self.actor.parameters())[0][0][:5]}")
```

### ステップ5: テスト専用スクリプトの作成

**問題**: 学習スクリプトとテストスクリプトが同じコードを使っている可能性

**推奨**: 独立したテストスクリプトを作成

```python
# test_ppo.py（新規作成）
def test_trained_model(model_path, test_episodes=20):
    """
    学習済みモデルをテスト環境で評価
    """
    # エージェントとモデルの初期化
    agent = PPOAgent(state_dim, action_dim, config)
    agent.load(model_path)
    
    # テスト環境の初期化（学習とは異なるデータ期間）
    test_env = EMSEnvironment(test_config)
    
    # 評価実行
    test_rewards = []
    for ep in range(test_episodes):
        state = test_env.reset()
        episode_reward = 0.0
        
        while True:
            # ★ force_teacher=False, deterministic=True でテスト
            action, _, _ = agent.select_action(
                state, 
                test_env.get_action_mask(), 
                deterministic=True  # 決定的選択
            )
            
            step_result = test_env.step(action)
            episode_reward += step_result.reward
            state = step_result.next_state
            
            if step_result.done:
                break
        
        test_rewards.append(episode_reward)
    
    return np.mean(test_rewards), test_env.get_episode_statistics()
```

---

## 🎓 根本原因の総括

### 文書で指摘された問題について

**✅ log_prob計算のバグ**: 既に修正済み（ppo_agent.py L216）  
**✅ optimal_action取得のタイミング**: 既に修正済み（trainer.py L214）  
**✅ advance_time()の実装**: 既に実装済み

### 実際の問題（推定）

1. **修正前のコードで学習したモデルを使用している**（可能性：高）
   - 対策: 修正後のコードで再学習

2. **テスト時の評価方法に問題がある**（可能性：中）
   - 対策: `force_teacher=False`で評価

3. **optimal_actionが実行時に正しく取得できていない**（可能性：中）
   - 対策: ログで確認、環境側の実装を確認

---

## ✅ 最終推奨事項

### 即座に実施すべきこと

1. **修正済みコードで再学習を実行**
   ```bash
   python train_ppo.py --config config.yaml --debug
   ```

2. **学習時のログを確認**
   - `optimal_action`がNoneでないか
   - 教師一致率が高いか
   - 学習が収束しているか

3. **評価方法を修正**
   - trainer.pyのL417を`force_teacher=False`に固定

4. **テスト専用スクリプトを作成**
   - 学習とは独立した評価環境で性能を測定

### 長期的な対策

1. **ログ記録の強化**
   - 各ステップでのoptimal_action取得状況
   - 教師使用率の統計
   - モデルパラメータの変化

2. **検証機能の追加**
   - モデル保存・読み込みの整合性チェック
   - 学習データとテストデータの分布比較
   - 過学習の検出

3. **テスト環境の整備**
   - 独立したテストデータセット
   - ベースライン手法との比較
   - 統計的有意性の検証

---

## 📝 結論

**提供された修正案の妥当性**: ✅ **妥当**（ただし既に実装済み）

**根本原因**: 
- 文書で指摘されたコードレベルのバグは既に修正されている
- 問題は、**修正前のコードで学習したモデルを使用している**、または**テスト時の評価方法に問題がある**可能性が高い

**次のアクション**:
1. 修正済みコードで再学習
2. 評価方法の見直し（force_teacher=Falseに変更）
3. ログとメトリクスの詳細確認
4. 必要に応じて環境側（ems_environment.py）の検証

このレポートに基づいて、体系的にデバッグを進めることをお勧めします。
