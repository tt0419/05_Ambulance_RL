# PPO学習設定（シンプル報酬版）
# 応答時間のみを最適化（最速・最確実）

experiment:
  name: "ppo_simple_reward"
  description: "シンプルな報酬設計：応答時間のみを最小化"
  seed: 42
  device: "cuda"

data:
  data_paths:
    grid_mapping: "data/tokyo/processed/grid_mapping_res9.json"
    travel_time_matrix: "data/tokyo/calibration2/linear_calibrated_response.npy"
  
  episode_duration_hours: 24
  exclude_daytime_ambulances: true
  
  area_restriction:
    enabled: true
    area_name: "東京23区"
    num_ambulances_in_area: 192
    state_dim: 999
    action_dim: 192
  
  train_periods:
    - start_date: "20230615"
      end_date: "20230621"
  
  eval_periods:
    - start_date: "20230622"
      end_date: "20230622"
  
  max_steps_per_episode: 3000

ppo:
  n_episodes: 200  # まず200エピソードでクイックテスト
  batch_size: 1024
  n_epochs: 6
  clip_epsilon: 0.1
  learning_rate:
    actor: 0.0003
    critic: 0.001
  gamma: 0.99
  gae_lambda: 0.95
  entropy_coef: 0.02  # 少し増やして探索を促進

reward:
  system:
    dispatch_failure: -1.0
    no_available_ambulance: 0.0
  
  core:
    mode: "simple"  # シンプルモード
    
    simple_params:
      # 🔥 シンプル設計：応答時間のみ
      time_penalty_per_minute: -1.0      # 1分 = -1ポイント
      
      # ボーナス
      critical_under_6min_bonus: 15.0    # 6分以内
      moderate_under_13min_bonus: 8.0    # 13分以内
      mild_under_13min_bonus: 8.0
      
      # ペナルティ
      over_13min_penalty: -10.0
      over_20min_penalty: -100.0         # 20分超過は極端なペナルティ
      
      # カバレッジは無視（ボーナス0）
      imitation_bonus: 0.0

# 🔥 学習時はハイブリッド無効（全事案で学習）
hybrid_mode:
  enabled: false

network:
  state_encoder:
    ambulance_features: 8
    incident_features: 6
    spatial_features: 16
    temporal_features: 8
  
  actor:
    hidden_layers: [256, 128]
    activation: "relu"
    dropout: 0.0  # ドロップアウト無効
  
  critic:
    hidden_layers: [256, 128]
    activation: "relu"
    dropout: 0.0

training:
  checkpoint_interval: 50
  keep_last_n: 3
  
  early_stopping:
    enabled: true
    patience: 30
    min_delta: 0.5
  
  logging:
    interval: 50
    tensorboard: true
    wandb: true
    wandb_project: "ems_simple_reward"

evaluation:
  interval: 25  # 頻繁に評価
  n_eval_episodes: 3
  compare_baselines: ["closest"]
  
  metrics:
    - "overall_mean_rt"
    - "mild_mean_rt"
    - "moderate_mean_rt"
    - "vs_closest_improvement"

# 🔥 模倣学習を強力に有効化
teacher:
  enabled: true
  strategy: "closest"
  initial_prob: 0.9      # 初期90%模倣（ほぼ教師通り）
  final_prob: 0.3        # 最終30%模倣
  decay_episodes: 150    # 150エピソードで減衰
  apply_to: ["軽症", "中等症", "重症", "重篤", "死亡"]

