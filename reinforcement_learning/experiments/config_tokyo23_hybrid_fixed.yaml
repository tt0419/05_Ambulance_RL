# PPO学習設定（修正版）
# 問題点を修正：応答時間を最優先に

experiment:
  name: "hybrid_ppo_tokyo23_fixed_v1"
  description: "報酬関数を修正：応答時間重視、カバレッジは補助的"
  seed: 42
  device: "cuda"

data:
  data_paths:
    grid_mapping: "data/tokyo/processed/grid_mapping_res9.json"
    travel_time_matrix: "data/tokyo/calibration2/linear_calibrated_response.npy"
  
  episode_duration_hours: 24
  exclude_daytime_ambulances: true
  
  area_restriction:
    enabled: true
    area_name: "東京23区"
    num_ambulances_in_area: 192
    state_dim: 999
    action_dim: 192
  
  # より多くの学習データ
  train_periods:
    - start_date: "20230615"
      end_date: "20230621"  # 1週間に拡張
  
  eval_periods:
    - start_date: "20230622"
      end_date: "20230622"
  
  max_steps_per_episode: 3000

ppo:
  n_episodes: 1000  # エピソード数を増加
  batch_size: 1024
  n_epochs: 6
  clip_epsilon: 0.1
  learning_rate:
    actor: 0.0003
    critic: 0.001
  gamma: 0.99
  gae_lambda: 0.95
  entropy_coef: 0.015

reward:
  system:
    dispatch_failure: -1.0
    no_available_ambulance: 0.0
  
  core:
    mode: "hybrid"
    hybrid_params:
      time_penalty_per_minute: -0.5  # 応答時間ペナルティ強化
      mild_under_13min_bonus: 10.0   # ボーナス増加
      moderate_under_13min_bonus: 15.0
      over_13min_penalty: -10.0      # ペナルティ強化
      over_20min_penalty: -100.0     # 極端な遅延に厳しいペナルティ
      good_coverage_bonus: 3.0       # カバレッジボーナス減少
      coverage_maintenance_bonus: 2.0
      poor_coverage_penalty: -3.0    # カバレッジペナルティ緩和
      balanced_workload_bonus: 1.0
      overloaded_penalty: -2.0

# 🔥 最重要：報酬の重み調整
hybrid_mode:
  enabled: false  # 学習時はハイブリッド無効（全事案で学習）
  severity_classification:
    severe_conditions: ["重症", "重篤", "死亡"]
    mild_conditions: ["軽症", "中等症"]
  
  reward_weights:
    response_time: 0.7      # 🔥 0.4 → 0.7（応答時間最優先）
    coverage: 0.2           # 🔥 0.5 → 0.2（カバレッジは補助）
    workload_balance: 0.1
  
  time_thresholds:
    good: 13
    warning: 20
  
  penalties:
    over_warning: -100.0    # 20分超過に厳しいペナルティ
    per_minute_over: -5.0

network:
  state_encoder:
    ambulance_features: 8
    incident_features: 6
    spatial_features: 16
    temporal_features: 8
  
  actor:
    hidden_layers: [256, 128]
    activation: "relu"
    dropout: 0.1
  
  critic:
    hidden_layers: [256, 128]
    activation: "relu"
    dropout: 0.1

training:
  checkpoint_interval: 100
  keep_last_n: 5
  
  early_stopping:
    enabled: true
    patience: 20
    min_delta: 1.0
  
  logging:
    interval: 100
    tensorboard: true
    wandb: true
    wandb_project: "ems_hybrid_fixed"

evaluation:
  interval: 100
  n_eval_episodes: 5
  compare_baselines: ["closest", "severity_based"]
  
  metrics:
    - "overall_mean_rt"      # 最重要メトリック
    - "mild_mean_rt"
    - "moderate_mean_rt"
    - "critical_6min_rate"
    - "overall_13min_rate"
    - "coverage_score"
    - "vs_closest_improvement"

# 模倣学習（オプション）
teacher:
  enabled: true              # 🔥 有効化
  strategy: "closest"        # 直近隊を教師に
  initial_prob: 0.5          # 初期は50%模倣
  final_prob: 0.1            # 最終は10%模倣
  decay_episodes: 200        # 200エピソードで減衰
  apply_to: ["軽症", "中等症"]

