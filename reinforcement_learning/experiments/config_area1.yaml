# config_area1_revised.yaml

# ★★★【修正点①】★★★
# どの設定ファイルをベースにするかを指定します。
# これにより、config.yaml の設定を読み込んだ上で、このファイルの内容で上書きします。
inherits: ./config.yaml

# ===================================================================
# 提案改善策を反映した第1方面用設定ファイル
# 主な戦略：
# 1. 【報酬の単純化】: 複雑な報酬体系を廃止し、「応答時間ペナルティ」を基本とするシンプルな構造に変更。
# 2. 【模倣学習の徹底】: 学習初期はほぼ100%直近隊を模倣し、安定したベースラインを確立する (Behavioral Cloningに近い)。
# 3. 【段階的な自律化】: カリキュラムを通じて、教師（直近隊）からの独立をゆっくりと促し、重症事案の改善のみを探索させる。
# 4. 【学習の安定化】: Criticの学習を優先し、ハイパーパラメータを安定志向で再調整。
# ===================================================================


# データパス設定（PPO戦略の初期化に必要）
data_paths:
  grid_mapping: "data/tokyo/processed/grid_mapping_res9.json"
  travel_time_matrix: "data/tokyo/calibration2/linear_calibrated_response.npy"



experiment:
  name: "ppo_area1_simple_reward_v1"
  description: "10000epまで引き延ばし、entropy_coefを0.015に増加"
  seed: 2025
  device: cuda

# -------------------------------------------------------------------
# データ設定 (変更なし)
# -------------------------------------------------------------------
data:
  area_restriction:
    enabled: true
    area_name: "第一方面"
    section_code: 1
    districts: ["千代田区", "中央区", "港区"]
    # ★★★ 次元数を明示的に定義（学習時と評価時の一貫性確保）★★★
    num_ambulances_in_area: 16  # 第1方面の救急車数
    state_dim: 118  # 学習時の状態次元数（16*5+10+8+20=118）
    action_dim: 16  # 学習時の行動次元数
  episode_duration_hours: 24
  train_periods:
    - start_date: "20230410"
      end_date: "20230414"
  eval_periods:
    - start_date: "20240408"
      end_date: "20240412"

# -------------------------------------------------------------------
# PPOハイパーパラメータ (安定性重視で再調整)
# -------------------------------------------------------------------
ppo:
  n_episodes: 10000
  batch_size: 512
  
  # 学習率: Criticを少し高めに設定し、価値関数の学習を促進
  learning_rate:
    actor: 0.00005  # 3e-5: 低めに維持
    critic: 0.0001 # 1e-4: Actorより少し高く設定
    scheduler: constant
  
  # PPOパラメータ: 更新を少しだけ許容
  n_epochs: 4
  clip_epsilon: 0.1     # 0.05から少し広げ、更新の停滞を防ぐ
  gamma: 0.99
  gae_lambda: 0.95
  entropy_coef: 0.015    # 探索の機会をわずかに増やす
  max_grad_norm: 0.5    # 勾配クリッピングを少し緩和

# -------------------------------------------------------------------
# 報酬設計 (【最重要】抜本的に単純化)
# -------------------------------------------------------------------
reward:
  # 基本方針: 応答時間(分)に応じたペナルティを主軸とし、報酬のスケールを[-5, +5]程度に正規化
  
  # 1. 中核となる時間ペナルティ
  time_penalty_per_minute: -0.25  # 1分あたり-0.25のペナルティ (例: 8分で-2.0)
  
  # 2. ポジティブな行動へのボーナス (報酬シェイピング)
  critical_under_6min_bonus: 10.0  # 重症系を6分以内で対応できたらボーナス
  imitation_bonus: 0.5            # 教師と同じ行動をしたら小さなボーナス
  
  # 3. 大きな失敗へのペナルティ
  over_13min_penalty: -2.0        # 13分を超えたら追加ペナルティ

  # 【廃止した項目】
  # - 複雑な傷病度別・時間段階別の報酬/ペナルティ
  # - エピソード数に応じた動的ボーナス
  # - 直近隊との乖離に対する直接的なペナルティ (imitation_bonusで代替)

# -------------------------------------------------------------------
# 教師あり学習 (模倣をさらに強化)
# -------------------------------------------------------------------
teacher:
  enabled: true
  strategy: closest  # 教師は直近隊戦略
  
  # カリキュラム: 最初はほぼ完全に模倣し、ゆっくり自律させる
  initial_prob: 0.99   # ほぼ100%模倣からスタート
  final_prob: 0.30     # 最終的にも30%は教師を参考にする
  decay_episodes: 5000 # 2500エピソードかけてゆっくり減衰

  # 【単純化】傷病度別の教師確率を廃止し、単一の減衰スケジュールに統一

# -------------------------------------------------------------------
# カリキュラム学習 (フェーズを単純化)
# -------------------------------------------------------------------
curriculum:
  enabled: true
  stages:
    # フェーズ1: 模倣による基礎学習 (0-1500エピソード)
    - episodes: [0, 1500]
      description: "徹底的な模倣フェーズ。teacher_probは高水準 (0.99 -> 0.6) を維持。"
      goals:
        - "vs_closest_improvement > -0.1" # ベースライン性能の90%以上を維持
        
    # フェーズ2: 誘導付き改善 (1500-3000エピソード)
    - episodes: [1500, 3000]
      description: "模倣を続けつつ(teacher_prob: 0.6 -> 0.3)、重症事案での改善を探索。"
      goals:
        - "critical_6min_rate > (baseline + 0.05)" # 重症6分率をベースラインから5%改善
        - "achieved_13min_rate > (baseline - 0.02)" # 全体13分率の悪化を2%以内に抑制

# -------------------------------------------------------------------
# ネットワーク & 評価設定 (変更なし)
# -------------------------------------------------------------------
network:
  actor:
    hidden_layers: [64, 32]
    activation: relu
    initialization: xavier_uniform
  critic:
    hidden_layers: [64, 32]
    activation: relu
    init_scale: 0.001 # Criticの初期出力をゼロに近づけ安定化

evaluation:
  interval: 100
  n_eval_episodes: 10
  compare_baselines: ["closest"]
  metrics:
    - "critical_6min_rate"
    - "achieved_13min_rate"
    - "mean_response_time"
    - "vs_closest_improvement"

# -------------------------------------------------------------------
# 学習管理
# -------------------------------------------------------------------
training:
  checkpoint_interval: 250
  logging:
    interval: 10
    wandb: true