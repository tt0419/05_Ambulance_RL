# 第1方面 - 直近隊運用の模倣から始める段階的改善
# 最近接戦略を教師として、重症系のみ徐々に改善

experiment:
  name: ppo_area1_imitation_v1
  description: "直近隊運用の模倣学習から段階的改善"
  seed: 2222
  device: cpu

data:
  area_restriction:
    enabled: true
    area_name: 第一方面
    section_code: 1
    districts:
      - 千代田区
      - 中央区
      - 港区
  
  # エピソードを長くして事案数を増やす
  episode_duration_hours: 24  # 24時間に延長
  
  train_periods:
    - start_date: "20230401"
      end_date: "20230415"  # 2週間分のデータ
  
  eval_periods:
    - start_date: "20230416"
      end_date: "20230420"

ppo:
  n_episodes: 3000  # 十分なエピソード数
  batch_size: 256   # 事案数増加に対応
  
  # 極めて保守的な学習率
  learning_rate:
    actor: 0.00001     # 非常に小さい学習率
    critic: 0.00005    # Criticも小さめ
    scheduler: constant
  
  # 保守的なPPOパラメータ
  n_epochs: 2          # エポック数を減らす
  clip_epsilon: 0.05   # クリップ範囲を狭める（保守的）
  gamma: 0.99
  gae_lambda: 0.95
  entropy_coef: 0.001  # 探索を最小限に
  value_loss_coeff: 1.0
  max_grad_norm: 0.1   # 勾配クリップを厳しく
  target_kl: 0.005     # KL divergenceを厳格に制限

# 報酬設計：直近隊との差分を最小化しつつ重症改善
reward:
  base_reward: 0.0
  
  # 1. 模倣報酬（最重要）
  imitation:
    match_closest: 10.0      # 最近接と同じ選択をしたら報酬
    deviation_penalty: -5.0  # 異なる選択をしたらペナルティ
    
    # ただし重症系では例外
    critical_exception:
      better_than_closest: 50.0  # 最近接より早く到着したら大報酬
      worse_than_closest: -10.0  # 最近接より遅いとペナルティ
  
  # 2. 応答時間報酬（傷病度別・段階的）
  response_time:
    critical:  # 重篤・重症・死亡（最重要）
      under_5min: 100.0   # 5分以内：大成功
      under_6min: 50.0    # 6分以内：成功
      under_8min: 20.0    # 8分以内：許容
      over_8min: -20.0    # 8分超：失敗
      
    moderate:  # 中等症
      under_10min: 10.0
      under_13min: 5.0
      over_13min: -5.0
      
    mild:  # 軽症（最近接に従う）
      follow_closest: 5.0   # 最近接と同じなら小報酬
      different: -2.0       # 異なる選択は小ペナルティ
  
  # 3. 改善ボーナス（段階的に増加）
  improvement_bonus:
    # エピソード数に応じて重症改善の重みを増やす
    episode_0_1000:
      critical_improvement: 10.0   # 初期は小さめ
    episode_1000_2000:
      critical_improvement: 30.0   # 中期で増加
    episode_2000_3000:
      critical_improvement: 50.0   # 後期で最大
  
  # 4. 全体性能維持
  performance_maintenance:
    below_baseline_13min: -20.0  # 13分達成率が基準以下
    maintain_baseline: 10.0      # 基準維持で報酬

# 教師あり学習（最近接戦略を教師とする）
teacher:
  enabled: true
  strategy: closest  # 最近接戦略を教師に
  
  # 段階的に独立性を増やす
  initial_prob: 0.95   # 最初はほぼ完全に模倣
  final_prob: 0.50     # 最終的にも50%は教師に従う
  decay_episodes: 2000 # ゆっくり減衰
  
  # 重症系では早期に独立
  severity_specific:
    critical_teacher_prob: 0.30  # 重症系は30%のみ教師
    mild_teacher_prob: 0.80      # 軽症は80%教師に従う
  
  adaptive_recovery:
    enabled: true
    check_interval: 50
    min_performance: 0.80  # 最近接の80%性能を維持

# カリキュラム学習（段階的目標）
curriculum:
  enabled: true
  stages:
    # Stage 1: 模倣学習（0-1000）
    - episodes: [0, 1000]
      focus: imitation
      goals:
        match_closest_rate: 0.90  # 90%は最近接と同じ
        critical_improvement: 0.0  # まだ改善を求めない
      
    # Stage 2: 重症系の探索開始（1000-2000）
    - episodes: [1000, 2000]
      focus: critical_exploration
      goals:
        match_closest_rate: 0.70  # 70%は最近接
        critical_6min_rate: 0.30  # 重症30%を6分以内に
      
    # Stage 3: 重症系の最適化（2000-3000）
    - episodes: [2000, 3000]
      focus: critical_optimization
      goals:
        match_closest_rate: 0.50  # 50%は最近接
        critical_6min_rate: 0.50  # 重症50%を6分以内に
        maintain_13min_rate: 0.85  # 全体性能維持

# ネットワーク（シンプルで安定）
network:
  actor:
    hidden_layers: [64, 32]
    activation: relu
    dropout: 0.0
    initialization: xavier_uniform  # 安定した初期化
    
  critic:
    hidden_layers: [64, 32]
    activation: relu
    dropout: 0.0
    init_scale: 0.001  # 非常に小さい初期値
    
  state_encoder:
    ambulance_features: 4
    incident_features: 10
    temporal_features: 8
    spatial_features: 20

# 評価設定
evaluation:
  interval: 50  # 50エピソードごと
  n_eval_episodes: 10  # より多くの評価
  eval_deterministic: true
  
  metrics:
    - critical_6min_rate
    - achieved_13min_rate
    - mean_response_time
    - vs_closest_improvement  # 最近接との比較
  
  compare_baselines:
    - closest  # 主要な比較対象
  
  # 成功基準
  success_criteria:
    critical_improvement: 0.1   # 重症系10%改善
    maintain_overall: 0.95      # 全体性能95%維持

training:
  checkpoint_interval: 100
  keep_last_n: 20
  
  # 安定性重視の設定
  gradient_accumulation: 4  # 勾配を蓄積して安定化
  
  early_stopping:
    enabled: false  # 長期学習のため無効
    
  logging:
    interval: 10
    log_level: INFO
    tensorboard: true
    wandb: true
    
    detailed_logs:
      - imitation_rate  # 模倣率
      - critical_performance
      - deviation_from_closest

advanced:
  # 安定化技術
  stabilization:
    gradient_clipping: true
    value_clipping: true
    advantage_normalization: true
    
  # 最近接との比較
  baseline_comparison:
    track_difference: true
    penalty_for_worse: true
    reward_for_better: true

metadata:
  description: |
    直近隊運用の模倣学習ベースアプローチ
    
    主要戦略：
    1. 最初は最近接戦略を95%模倣
    2. 段階的に重症系のみ改善を試みる
    3. 軽症・中等症は最近接に従う
    4. 全体性能を維持しつつ重症改善
    
    3段階カリキュラム：
    - Stage 1: 完全模倣（基礎学習）
    - Stage 2: 重症系の探索開始
    - Stage 3: 重症系の最適化
    
  notes: |
    - 24時間エピソードで事案数確保（約100件/日）
    - 極めて保守的な学習率で安定性重視
    - 最近接の良さを維持しつつ改善
    - 重症系（全体の10%）のみに焦点
    - 3000エピソードで十分な学習機会