# 改善版1日学習設定
experiment:
  name: "ppo_1day_improved"
  seed: 42
  device: "cuda"

# データパス設定（PPO戦略の初期化に必要）
data_paths:
  grid_mapping: "data/tokyo/processed/grid_mapping_res9.json"
  travel_time_matrix: "data/tokyo/calibration2/linear_calibrated_response.npy"

data:
  # ★★★ 全地域用の次元数設定（後方拡張性確保）★★★
  area_restriction:
    enabled: false  # 全地域使用
    num_ambulances_in_area: 192  # 全地域の救急車数
    state_dim: 998  # 全地域の状態次元数（192*5+10+8+20=998）
    action_dim: 192  # 全地域の行動次元数
  train_periods:
    - start_date: "20230401"
      end_date: "20230401"
  eval_periods:
    - start_date: "20230402"
      end_date: "20230402"
  episode_duration_hours: 6  # 6時間

# 傷病度設定（変更なし）
severity:
  categories:
    critical:
      conditions: ["重篤", "重症", "死亡"]
      reward_weight: 5.0
      time_limit_seconds: 360
    moderate:
      conditions: ["中等症"]
      reward_weight: 2.0
      time_limit_seconds: 480
    mild:
      conditions: ["軽症"]
      reward_weight: 1.0
      time_limit_seconds: 780
  thresholds:
    golden_time: 360
    standard_time: 780

# PPOハイパーパラメータ（改善版）
ppo:
  n_episodes: 50 # 500 → 50
  n_epochs: 5
  batch_size: 32
  clip_epsilon: 0.1  # 0.2 → 0.1
  gamma: 0.99
  gae_lambda: 0.95
  learning_rate:
    actor: 0.0001  # 3e-4 → 1e-4
    critic: 0.0001  # 3e-4 → 1e-4
    scheduler: "cosine"
  entropy_coef: 0.01

# 報酬関数設定（緩和版）
reward:
  weights:
    response_time: -0.5  # -1.0 → -0.5
    severity_bonus: 3.0  # 2.0 → 3.0
    threshold_penalty: -5.0  # -10.0 → -5.0
    coverage_preservation: 0.5
  penalties:
    over_6min: -2.5  # -5.0 → -2.5
    over_13min: -10.0  # -20.0 → -10.0
    per_minute_over: -0.5  # -1.0 → -0.5

# 教師あり学習設定（新規追加）
teacher:
  enabled: true
  initial_prob: 0.8  # 初期教師使用率
  final_prob: 0.2  # 最終教師使用率
  decay_episodes: 1000  # 減衰期間

evaluation:
  interval: 50
  n_eval_episodes: 5