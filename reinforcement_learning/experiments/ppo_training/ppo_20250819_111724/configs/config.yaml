advanced:
  adaptive_learning:
    batch_size_adaptation: true
    enabled: true
    learning_rate_adaptation: true
  memory_replay:
    buffer_size: 10000
    enabled: false
    sample_ratio: 0.1
  multi_objective:
    enabled: true
    objectives:
    - response_time
    - equity
    - resource_efficiency
    weights:
    - 0.6
    - 0.25
    - 0.15
data:
  episode_duration_hours: 12
  eval_periods:
  - end_date: '20230409'
    start_date: '20230408'
  - end_date: '20230418'
    start_date: '20230417'
  - end_date: '20230428'
    start_date: '20230427'
  train_periods:
  - end_date: '20230407'
    start_date: '20230401'
  - end_date: '20230416'
    start_date: '20230410'
  - end_date: '20230426'
    start_date: '20230420'
evaluation:
  compare_baselines:
  - closest
  - severity_based
  - advanced_severity
  detailed_analysis_interval: 100
  interval: 25
  metrics:
  - mean_response_time
  - response_time_by_severity
  - 6min_achievement_rate
  - 13min_achievement_rate
  - critical_6min_rate
  - resource_utilization
  - spatial_coverage
  - temporal_distribution
  - severity_specific_performance
  n_eval_episodes: 10
  stress_tests:
    enabled: true
    scenarios:
    - high_demand_peak
    - emergency_clustering
    - resource_shortage
experiment:
  device: cpu
  name: ppo_scaled_experiment
  seed: 42
network:
  actor:
    activation: relu
    batch_norm: true
    dropout: 0.15
    hidden_layers:
    - 512
    - 256
    - 128
    - 64
  critic:
    activation: relu
    batch_norm: true
    dropout: 0.15
    hidden_layers:
    - 512
    - 256
    - 128
  state_encoder:
    ambulance_features: 12
    incident_features: 8
    spatial_features: 24
    temporal_features: 12
ppo:
  batch_size: 64
  clip_epsilon: 0.15
  entropy_coef: 0.02
  gae_lambda: 0.95
  gamma: 0.99
  learning_rate:
    actor: 0.0003
    critic: 0.0005
    scheduler: cosine_with_restarts
  max_grad_norm: 0.5
  n_episodes: 2000
  n_epochs: 8
  value_coef: 0.5
regularization:
  action_noise:
    decay_episodes: 1000
    enabled: true
    final_std: 0.01
    initial_std: 0.1
  exploration_bonus:
    coefficient: 0.1
    enabled: true
  weight_decay: 0.0001
reward:
  bonuses:
    resource_efficiency: 2.0
    under_3min_critical: 8.0
    under_6min_critical: 5.0
  penalties:
    over_13min: -15.0
    over_6min: -3.0
    per_minute_over: -0.8
    resource_waste: -0.2
  weights:
    coverage_preservation: 1.0
    efficiency_bonus: 0.3
    response_time: -0.8
    severity_bonus: 4.0
    threshold_penalty: -8.0
severity:
  categories:
    critical:
      conditions:
      - 重篤
      - 重症
      - 死亡
      reward_weight: 5.0
      time_limit_seconds: 360
    mild:
      conditions:
      - 軽症
      reward_weight: 1.0
      time_limit_seconds: 780
    moderate:
      conditions:
      - 中等症
      reward_weight: 2.0
      time_limit_seconds: 480
  thresholds:
    golden_time: 360
    standard_time: 780
teacher:
  decay_episodes: 800
  decay_schedule: exponential
  enabled: true
  expert_strategies:
  - severity_based
  - closest
  final_prob: 0.1
  initial_prob: 0.9
training:
  checkpoint_interval: 50
  curriculum:
    enabled: true
    stages:
    - difficulty: easy
      episode_duration_hours: 6
      episodes: 500
    - difficulty: medium
      episode_duration_hours: 9
      episodes: 1000
    - difficulty: hard
      episode_duration_hours: 12
      episodes: 500
  early_stopping:
    enabled: true
    min_delta: 0.0005
    patience: 200
    restore_best_weights: true
  keep_last_n: 10
  logging:
    interval: 5
    save_model_every: 100
    tensorboard: true
    wandb: true
