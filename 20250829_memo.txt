件名: PPO学習における課題の特定と新戦略への転換に関するサマリー
日付: 2025年8月29日

概要
PPOを用いた救急隊配車最適化の学習が停滞している問題に対し、体系的なデバッグと実験を繰り返した。初期の学習が完全に失敗していた原因として、①報酬設計の複雑さ、②設定ファイルの不整合、③評価ロジックのバグ、④ベースライン比較の不整合など、複数の根本的な問題を特定し、これらを段階的に解消した。

修正の結果、エージェントがベースライン戦略を正確に模倣できることを確認し、信頼性のある訓練・評価基盤を確立した。しかし、その後の改善を試みる学習において、PPOの探索的アプローチでは強力なベースラインを超えることが極めて困難であると判明。この知見に基づき、今後は「ベースラインを補助するハイブリッド戦略」へと方針を転換することを決定した。

発見した主な問題点
初期学習の完全な失敗: 当初、Criticの損失が発散し、報酬が全く改善しない状態だった。これは、報酬設計が過度に複雑で罰則が強すぎたため、学習プロセス自体が機能していなかった。

設定とコードの不整合:

ハードコードされたロジック: trainer.py内で教師あり学習の確率が固定値で記述されており、設定ファイルのカリキュラムが無視されていた。

評価ロジックのバグ: 評価時には教師機能が無効化され、訓練時と評価時でエージェントの振る舞いが異なっていたため、模倣実験の正しい評価ができていなかった。

ベースライン比較の誤り（モノサシの不一致）:

RL環境のエージェントの報酬を、高精度シミュレータ（validation_simulation.py）で算出されたと思われる静的なスコア (-3200) と比較していた。両者は算出基準が全く異なり、この比較は無意味だった。

PPOの探索能力の限界:

学習基盤を整備した後、エージェントに自律的な探索を許可したところ、性能がベースライン（直近隊戦略）を大幅に下回った。これは、ベースラインが非常に強力であり、単純な探索ではそれを超える高度な戦略（例：将来の需要予測に基づく配車）を発見できないことを示唆している。

得られた知見・決定事項
訓練環境と評価環境の分離:

訓練用環境 (EMSEnvironment): 学習速度を重視した単純化モデル。エージェントの訓練と、その環境内での相対的な性能評価に使用する。

最終評価用環境 (validation_simulation.py): 現実との近さを重視した高精度モデル。訓練済みのエージェントの最終的な実力を、現実的な指標（6分達成率など）で評価するために使用する。

この2つの役割を明確に分離して開発を進めることを決定した。

信頼できるベースラインの確立:

EMSEnvironment内で直近隊戦略を直接実行し、その平均報酬 (-1093.92) を計測した。これを、PPOエージェントが訓練環境で目指すべき真のベースラインスコアとして確立した。

模倣能力の検証完了:

上記で確立した真のベースラインに対し、教師確率100%でエージェントを学習させた結果、ほぼ同等の報酬スコア (-1099.95) を達成。これにより、エージェントがベースラインを正確に学習・模倣する能力を持つことが完全に証明された。

戦略の転換（ハイブリッド戦略の採用）:

エージェントにゼロから最適配車を考えさせるのではなく、「優れた直近隊戦略を、いつ、どのように覆すべきか」という、より高度な意思決定のみを学習させる方針へと転換する。

次のステップ
ハイブリッド戦略の実装:

エージェントの行動空間を「{0: 直近隊戦略に従う, 1: 2番目に近い隊を選ぶ, ...}」のような少数の選択肢に変更する。

EMSEnvironmentを修正し、この新しい行動空間に対応させる。

報酬設計の再調整:

エージェントがベースラインを覆す判断 (action > 0) を行い、かつそれがシステム全体で良い結果（例: 将来の重症事案への対応改善）につながった場合にのみ、大きな正の報酬を与えるように報酬を調整する。

新戦略での学習実行:

単純化された新しい問題設定で、エージェントが「例外的な良手」を学習できるかを検証する。