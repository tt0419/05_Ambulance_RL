========================================
救急隊ディスパッチ最適化プロジェクト
PPO実装フェーズ 第3方面限定実装記録
========================================

実施日: 2025年8月22日
実施者: プロジェクトチーム
フェーズ: Phase 3b - PPO学習収束問題対策・第3方面限定実装

----------------------------------------
【実施内容】
----------------------------------------

1. 前回(8/18)からの課題
   - PPO学習が収束しない（平均報酬-6110、6分達成率4.2%）
   - Critic損失が異常値（17,375）
   - 東京23区全体（192台）では規模が大きすぎる
   - action_mask全Falseエラーの頻発

2. 実装した解決策

   2.1 第3方面限定実装
       - データ制限: hanso_special_wards.csv の '出場先区市' を目黒区・渋谷区・世田谷区に限定
       - 救急車制限: amb_place_master.csv の section=3 に限定（25台）
       - 除外対象: team_name='救急隊なし' および 'デイタイム' 含む隊を除外
       - エピソード短縮: 24時間 → 8時間

   2.2 動的次元対応システム
       - 固定192台 → 環境依存の動的サイズ（25台）
       - train_ppo.py: action_dim = env.action_dim に修正
       - buffer.py: action_dim を __init__ で受け取り、動的にaction_masks初期化
       - state_encoder.py: max_ambulances を動的設定
       - ppo_agent.py: action_dim をRolloutBufferに渡す

   2.3 現実的な活動時間システム統合
       - 固定60分復帰 → validation_simulation.py互換の多段階活動時間
       - ServiceTimeGenerator統合（on_scene_time, hospital_time）
       - HierarchicalServiceTimeGenerator作成（lognormal_parameters_hierarchical.json対応）
       - _calculate_ambulance_completion_time実装（応答・現場・搬送・病院・帰署の全段階）
       - _select_hospital実装（距離ベースの病院選択）

   2.4 他地域応援システム実装
       - 重症度別最大待機時間: 重症10分、中等症20分、軽症45分
       - _handle_unresponsive_call実装:
         * 重症 → 緊急応援（15分追加で対応）
         * 中等症 → 標準応援（25分追加で対応）
         * 軽症 → 遅延応援 or 搬送見送り（60分超は自力搬送）
       - 詳細統計収集: critical_emergency_support, moderate_standard_support等

   2.5 数値安定性向上
       - ppo_agent.py: マスキング処理強化、1e-8最小値設定、NaN対策
       - buffer.py: action_mask妥当性チェック、全False警告・修正
       - network_architectures.py: xavier_uniform_重み初期化
       - state_encoder.py: 座標正規化改善、np.nan_to_num追加

3. 実験設定（config_area3.yaml）
   - エピソード数: 500エピソード（収束確認用）
   - バッチサイズ: 64（第3方面の事案数に適合）
   - 学習率: actor 3e-4, critic 5e-4
   - 教師あり学習: 初期80% → 最終30%（200エピソードで減衰）
   - エントロピー係数: 0.08（限定選択肢での探索促進）

----------------------------------------
【実験結果】
----------------------------------------

【第3方面環境確認】
期間                    救急車    事案数/8h    負荷率    特記事項
実行時                  25台      60-80件      52.5%     適正範囲

【PPO学習結果（500エピソード完了）】
指標                    結果値         目標値        達成状況
平均報酬                -283.30        正の値        ❌ 依然負
6分達成率              15-30%         40-50%        ❌ 未達成
13分達成率             未測定         85-90%        - 
vs closest             -90-92%悪化    改善期待      ❌ 大幅劣化
vs severity_based      -90.8%悪化     改善期待      ❌ 大幅劣化

【action_mask警告状況】
警告発生                大量発生       解消期待      ❌ 継続発生
原因                    救急車飽和     他地域応援    ✅ 原因特定
対策効果                他地域応援発動 統計のみ      ⚠️ 学習には未反映

【他地域応援システム動作確認】
重症緊急応援            発動確認       -             ✅ 正常動作
中等症標準応援          発動確認       -             ✅ 正常動作  
軽症搬送見送り          発動確認       -             ✅ 正常動作

----------------------------------------
【得られた知見】
----------------------------------------

1. 成功要因
   - 第3方面限定により管理可能な規模に縮小成功
   - 動的次元対応でエラー解消
   - 現実的活動時間の統合完了
   - 他地域応援システムの実装完了
   - 数値安定性の大幅改善（NaN/エラー解消）

2. 未解決課題
   - PPO学習の収束性（500エピソードでも改善なし）
   - 報酬設計の問題（基本報酬が負のまま）
   - 救急車飽和下での最適化困難
   - 教師確率30%でもベースライン比90%劣化

3. 技術的発見
   - action_mask警告は救急車不足の現実的制約
   - 現実的活動時間（90-150分）により事案処理能力大幅低下
   - 25台×8時間vs60-80事案は限界的負荷
   - 他地域応援は統計的には機能するが学習には寄与せず

4. 重要な洞察
   - 第3方面でも救急車不足は深刻（現実的制約）
   - 単純な最寄り戦略が実は高度に最適化済み
   - PPOが複雑な制約下で既存手法を上回るのは困難
   - 「救急車不足下での最適化」が新たな研究課題

----------------------------------------
【ファイル構成変更】
----------------------------------------

新規作成:
- reinforcement_learning/experiments/config_area3.yaml（第3方面設定）
- test_realistic_ambulance_times.py（活動時間検証、後に削除）

主要修正:
- ems_environment.py: 
  * _load_base_data（第3方面フィルタ）
  * _calculate_ambulance_completion_time（現実的活動時間）
  * _handle_unresponsive_call（他地域応援）
  * HierarchicalServiceTimeGenerator（階層的パラメータ対応）
- data_cache.py: area_filter パラメータ追加
- train_ppo.py: action_dim動的取得
- buffer.py: action_dim動的対応、全False警告
- ppo_agent.py: 堅牢なマスキング・NaN対策
- state_encoder.py: max_ambulances動的設定

削除ファイル:
- test_area3_environment.py, test_enhanced_logging.py等（検証完了）
- config_quick.yaml, config_test.yaml等（統合済み）

実行コマンド例:
```python
# 第3方面PPO学習（実行済み）
python train_ppo.py --config reinforcement_learning/experiments/config_area3.yaml
```

----------------------------------------
【次のチャットでの作業指針】
----------------------------------------

1. 報酬設計の根本見直し
   - 基本報酬を正の値に設定（+10.0など）
   - ペナルティの緩和（現在-5.0 → -2.0程度）
   - 相対評価から絶対評価への転換

2. 学習アプローチの変更
   - カリキュラム学習: より簡単な問題から段階的に
   - 模倣学習強化: 教師確率を段階的に下げる
   - ハイブリッド戦略: ルールベース+強化学習

3. 評価指標の再定義
   - ベースライン比較ではなく絶対性能評価
   - 救急車不足制約下での相対改善度
   - 他地域応援発動率の最小化

4. 研究方向性の転換案
   - 「完全自動化」から「人間支援システム」へ
   - 「個別最適化」から「システム全体最適化」へ
   - 「学習性能」から「実運用適合性」へ

----------------------------------------
【優先検討事項】
----------------------------------------

1. PPO学習の抜本的改善（報酬正規化、学習率調整）
2. 救急車不足制約下での新評価指標定義
3. 段階的学習アプローチの検討
4. 研究の学術的貢献の再定義

【現状の技術的成果】
✅ 完了項目:
- 現実的な救急システムの完全実装
- 他地域応援による対応不能事案処理
- 第3方面限定による管理可能な規模
- 数値安定性とエラー解消

⚠️ 要改善:
- PPO学習の収束性（根本的課題）
- 報酬設計の妥当性
- ベースライン比較での劣化

🎯 新たな目標:
- 救急車不足制約下での最適化研究
- 他地域応援最小化を目指す学習
- 現実運用に即した評価指標確立

========================================
記録終了
========================================
