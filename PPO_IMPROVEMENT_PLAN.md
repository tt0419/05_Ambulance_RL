# PPO改善計画

## 📊 問題の要約

### 現状の性能
```
軽症系: 14.33分（PPO） vs 7.33分（直近隊） → +7.00分 (+95.5%)
中等症: 14.81分（PPO） vs 7.61分（直近隊） → +7.20分 (+94.6%)
```

### 根本原因
1. **カバレッジ重視の報酬設計**
   - カバレッジ: 50%、応答時間: 40%
   - PPOは遠い隊を派遣してカバレッジを維持
   
2. **学習データの偏り**
   - ハイブリッドモードで学習 → 軽症系のみ
   - 1日分のデータのみ

3. **模倣学習未使用**
   - 直近隊の良い行動を学習していない

---

## 🚀 改善案

### **案1: 報酬関数の修正のみ（最速）** ⭐⭐⭐

**メリット:**
- 実装時間: 10分
- 学習時間: 2-4時間
- 効果: 中～高

**実装手順:**

1. 新しい設定ファイルを使用して再学習
   ```bash
   python train_ppo.py --config reinforcement_learning/experiments/config_tokyo23_hybrid_fixed.yaml
   ```

2. 主な変更点:
   - 応答時間重み: 0.4 → 0.7
   - カバレッジ重み: 0.5 → 0.2
   - 学習時ハイブリッド無効化
   - 模倣学習有効化

**期待される改善:**
```
軽症系: 14.33分 → 8-9分（目標: 直近隊の7.33分に近づける）
```

---

### **案2: 直近隊戦略に戻す（暫定対応）** ⭐⭐

**メリット:**
- 実装時間: 5分
- 効果: 即座に最適性能

**デメリット:**
- PPOの学習成果を放棄

**実装手順:**

`baseline_comparison.py`の設定を変更:
```python
'ppo_agent': {
    'model_path': '...',
    'hybrid_mode': True,
    'severe_conditions': ['重症', '重篤', '死亡', '軽症', '中等症'],  # 全事案を重症扱い
    'mild_conditions': []  # 空にする
}
```

→ 全事案で直近隊運用

---

### **案3: 段階的改善（推奨・長期）** ⭐⭐⭐⭐⭐

**Phase 1: 模倣学習（1週間）**
1. 直近隊戦略を教師として学習
2. 初期は80%模倣、徐々に減らす
3. 基本的な近い隊選択を学習

**Phase 2: 報酬調整（1週間）**
4. 応答時間最優先の報酬関数
5. より多くのデータで学習（1ヶ月分）

**Phase 3: 高度な最適化（2週間）**
6. カバレッジ、公平性も考慮
7. マルチタスク学習

**期待される改善:**
```
軽症系: 14.33分 → 7.0-7.5分（直近隊同等またはそれ以上）
```

---

## 🔧 今すぐ実行できるアクション

### アクション1: 分析スクリプト実行

```bash
python analyze_ppo_root_cause.py
```

→ 学習設定の詳細確認

### アクション2: 再学習（案1）

```bash
# 新しい設定で学習
python train_ppo.py --config reinforcement_learning/experiments/config_tokyo23_hybrid_fixed.yaml

# 学習完了後、検証
python baseline_comparison.py --use-new-model
```

### アクション3: 暫定対応（案2）

`baseline_comparison.py`を編集して全事案を直近隊運用

---

## 📈 改善効果の予測

### シナリオA: 報酬修正のみ（案1）
```
軽症系: 14.33分 → 9.0分（-5.3分改善、残り+1.7分）
改善率: 約75%
```

### シナリオB: 模倣学習併用（案3-Phase1）
```
軽症系: 14.33分 → 7.8分（-6.5分改善、残り+0.5分）
改善率: 約90%
```

### シナリオC: 完全最適化（案3-Phase3）
```
軽症系: 14.33分 → 7.0分（-7.3分改善、直近隊同等）
改善率: 100%
カバレッジ・公平性も向上
```

---

## ⚠️ 注意事項

### 学習時の確認ポイント
1. **応答時間の短縮**
   - エピソードごとに平均応答時間を記録
   - 減少傾向があるか確認

2. **直近隊との比較**
   - 評価時に直近隊戦略と比較
   - 目標: 90%以上の性能

3. **過学習の防止**
   - 検証データで性能を確認
   - Early Stoppingを使用

### 学習パラメータ
- エピソード数: 500-1000
- 学習時間: 2-8時間（GPUあり）
- メモリ: 8-16GB RAM推奨

---

## 📝 次のステップ

1. **今日**: 案1の実装（新しい設定で再学習開始）
2. **明日**: 学習結果の確認、性能評価
3. **1週間後**: 案3-Phase1の実装（模倣学習）

---

## 🎯 成功基準

### 最低基準
```
軽症系: 14.33分 → 10.0分以下（-4.3分改善）
```

### 目標基準
```
軽症系: 14.33分 → 8.0分以下（-6.3分改善）
直近隊の7.33分に近づく
```

### 理想基準
```
軽症系: 直近隊と同等またはそれ以上
カバレッジ、公平性も向上
```

---

**推奨アクション: 案1を今すぐ実行 → 結果を見て案3へ進む**

