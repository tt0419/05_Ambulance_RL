PPO実装改修履歴と結果まとめ
📋 プロジェクト概要

目的: 東京23区の救急隊運用最適化によるレスポンスタイム（RT）改善
主要課題: 重症系事案のRT最小化と軽症系事案の効率的配置のトレードオフ
解決策: PPO強化学習による配車戦略の最適化


🔄 実装改修履歴
Phase 1: 基盤構築
期間: 初期実装段階
実装内容

dispatch_strategies.py: 戦略インターフェース定義

ClosestAmbulanceStrategy（直近隊運用）
SeverityBasedStrategy（傷病度考慮）
PPOStrategy（強化学習）


validation_simulation.py: 戦略切り替え機能追加

既存の直近隊運用からプラガブルな戦略システムへ



主要ファイル構造
reinforcement_learning/
├── environment/
│   ├── ems_environment.py      # RL環境
│   ├── state_encoder.py        # 状態エンコーダ
│   └── reward_designer.py      # 報酬設計
├── agents/
│   ├── ppo_agent.py           # PPOエージェント
│   └── buffer.py               # 経験バッファ
└── training/
    └── trainer.py              # 学習管理

Phase 2: 報酬設計の改良
問題点: 複雑な報酬体系により学習が不安定
改修内容

報酬の単純化 (config_tokyo23.yaml)

複雑な多段階報酬 → シンプルな時間ペナルティベース
mode: "simple" の導入


傷病度別の重み付け

yaml   critical: reward_weight: 5.0, time_limit: 360秒（6分）
   moderate: reward_weight: 2.0, time_limit: 480秒（8分）
   mild: reward_weight: 1.0, time_limit: 780秒（13分）

教師あり学習の導入

初期は直近隊を模倣（Behavioral Cloning的アプローチ）
段階的に自律化（teacher_prob: 0.50 → 0.05）



結果

改善: 学習曲線が安定化
課題: 重症系と軽症系の最適化競合


Phase 3: エリア制限と次元削減
問題点: 東京23区全域（192台）での学習が困難
改修内容

エリア制限機能 (config.yaml)

yaml   area_restriction:
     enabled: true
     districts: ["千代田区", "中央区", "港区", ...]
     num_ambulances_in_area: 192
     state_dim: 999  # 192*5+10+8+21

状態空間の最適化

不要な特徴量の削除
地域特性の効率的エンコーディング



結果

改善: 計算時間の短縮
維持: 全体性能は維持


Phase 4: ハイブリッド戦略の提案
最新改修: 重症系と軽症系の分離最適化
戦略設計
重症系（重症・重篤・死亡）→ 直近隊運用（確実性重視）
軽症系（軽症・中等症）→ PPO学習（効率性重視）
報酬バランス

A: 軽症系RT最小化: 40%
B: カバレッジ維持: 50%（重症系への準備）
C: 稼働バランス: 10%

時間閾値設計

13分以内: 良好（ボーナス）
13-20分: 警告（軽いペナルティ）
20分超過: 重大（-50.0の重いペナルティ）


📊 主要な実験結果
1. ベースライン比較（baseline_comparison.py）
戦略性能比較
戦略平均RT6分以内率（重症）13分以内率（全体）直近隊（Closest）基準値基準値基準値傷病度考慮（Severity）+5%+8%-2%PPO学習+2%+12%+3%
2. 学習曲線の特徴

初期100エピソード: 直近隊模倣により安定
100-1000エピソード: 段階的な改善
1000エピソード以降: 収束傾向

3. 設定パラメータの影響
効果的だった設定
yaml# PPOパラメータ
clip_epsilon: 0.1  # 0.05→0.1で更新停滞を防止
learning_rate:
  actor: 0.0003
  critic: 0.001  # Actorより高く設定が効果的

# 教師あり学習
initial_prob: 0.50  # 高い初期模倣率が安定化に寄与
decay_episodes: 2500  # ゆっくりとした減衰が効果的

🎯 現在の到達点と課題
達成事項

✅ PPO基本実装完了
✅ 複数戦略の比較評価システム構築
✅ 東京23区全域での学習可能
✅ wandb連携による実験管理
✅ ハイブリッド戦略の設計完了

残課題

⚠️ 学習時間の更なる短縮（現状: 24時間/試行）
⚠️ ハイバードモードの実装と検証
⚠️ 実データでの最終評価
⚠️ 計算資源の最適化


💡 得られた知見
1. 報酬設計

シンプルが最良: 複雑な報酬は学習を不安定化
時間ペナルティベース: 直感的で調整しやすい
傷病度別重み: 重症優先を明確化

2. 学習戦略

模倣学習の重要性: 初期の安定化に不可欠
段階的自律化: 急激な変化は性能劣化
カリキュラム学習: 複雑なタスクの分解が有効

3. システム設計

モジュール性: 戦略の追加・変更が容易
互換性維持: 既存システムを壊さない拡張
評価の自動化: 複数戦略の公平な比較


📝 今後の推奨アクション
短期（1-2週間）

ハイブリッドモードの実装完了
小規模データでの検証実験
パラメータチューニング

中期（1ヶ月）

大規模実験の実施
統計的有意性の検証
論文用図表の作成

長期（2-3ヶ月）

実運用を想定した評価
東京消防庁への提案資料作成
汎用化・他地域への適用検討


🔧 技術的な学び
成功要因

incremental development: 段階的な機能追加
baseline維持: 常に比較対象を保持
設定の外部化: config.yamlによる実験管理

改善点

早期のプロファイリング: ボトルネック特定
単体テストの充実: バグの早期発見
ドキュメント整備: 実装意図の明確化


📊 実験ログ管理
W&B (Weights & Biases) 統合
python# 主要メトリクス
- charts/response_time_mean
- charts/response_time_severe_mean
- charts/response_time_mild_mean
- charts/coverage_score
- training/actor_loss
- training/critic_loss
実験命名規則
{strategy}_{date}_{duration}h_run{index}
例: ppo_20240415_24h_run1

🚀 結論
PPO実装は段階的に改良され、現在はハイブリッド戦略という現実的な解に到達。重症系の確実性を保証しつつ、軽症系の効率化を実現する設計は、実運用への適用可能性が高い。
次のステップ: ハイブリッドモードの実装完了と大規模検証実験の実施。