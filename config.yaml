# config.yaml - すべての調整可能パラメータを一元管理
# ================================================================
# 実験設定
# ================================================================
experiment:
  name: "ppo_dispatch_optimization"
  seed: 42
  device: "cuda"  # "cpu" or "cuda"
  
# ================================================================
# データ設定（調整頻度：高）
# ================================================================
data:
  # 学習用データ期間
  train_periods:
    - start_date: "20230401"  # ← 変更箇所：学習開始日
      end_date: "20230430"    # ← 変更箇所：学習終了日
    - start_date: "20231201"  # 繁忙期データも含める
      end_date: "20231230"
  
  # 評価用データ期間  
  eval_periods:
    - start_date: "20230501"  # ← 変更箇所：評価開始日
      end_date: "20230507"    # ← 変更箇所：評価終了日
  
  # シミュレーション設定
  episode_duration_hours: 24  # ← 変更箇所：1エピソードの長さ（時間）

# ================================================================
# 評価時に必要となるデータパスの設定（調整頻度：低）
# ================================================================
data_paths:
  grid_mapping: "data/tokyo/processed/grid_mapping_res9.json"
  travel_time_matrix: "data/tokyo/calibration2/linear_calibrated_response.npy"

# ================================================================
# 傷病度設定（調整頻度：中）
# ================================================================
severity:
  # 傷病度カテゴリと重み（重症・重篤・死亡は同じ重み）
  categories:
    critical:
      conditions: ["重症", "重篤", "死亡"]  # ← 順序を統一（baseline_comparison.pyと同じ）
      reward_weight: 5.0  # ← 変更箇所：重症系の重み
      time_limit_seconds: 360  # 6分目標
      
    moderate:
      conditions: ["中等症"]
      reward_weight: 2.0  # ← 変更箇所：中等症の重み
      time_limit_seconds: 480  # 8分目標
      
    mild:
      conditions: ["軽症"]
      reward_weight: 1.0  # ← 変更箇所：軽症の重み
      time_limit_seconds: 780  # 13分以内
      
  # 時間閾値設定
  thresholds:
    golden_time: 360  # 6分（重症系目標）← 変更箇所
    standard_time: 780  # 13分（全体目標）← 変更箇所

# ================================================================
# PPOハイパーパラメータ（調整頻度：高）
# ================================================================
ppo:
  # 学習設定
  n_episodes: 10000  # ← 変更箇所：総エピソード数
  n_epochs: 10  # ← 変更箇所：PPO更新エポック数
  batch_size: 64  # ← 変更箇所：バッチサイズ
  
  # PPOアルゴリズムパラメータ
  clip_epsilon: 0.2  # ← 変更箇所：クリッピング範囲
  gamma: 0.99  # ← 変更箇所：割引率
  gae_lambda: 0.95  # ← 変更箇所：GAEパラメータ
  
  # 学習率設定
  learning_rate:
    actor: 0.0003  # ← 変更箇所：Actor学習率
    critic: 0.0003  # ← 変更箇所：Critic学習率
    scheduler: "cosine"  # "constant", "linear", "cosine"
    
  # エントロピー正則化
  entropy_coef: 0.01  # ← 変更箇所：探索促進

# ================================================================
# 報酬関数設定（調整頻度：高）
# ================================================================
reward:
  # システムレベル：環境固有の基本動作（通常変更しない）
  system:
    dispatch_failure: -1.0           # 配車失敗時のペナルティ
    no_available_ambulance: 0.0      # 利用可能な救急車なし
    unhandled_call_penalty: -1.0     # 対応不能事案の基本ペナルティ
    
  # コアレベル：学習の中核となる報酬（実験で調整）
  core:
    mode: "continuous"  # "continuous", "discrete", "simple"
    
    # 連続報酬モード用パラメータ
    continuous_params:
      critical:
        target: 6           # 目標時間（分）
        max_bonus: 50.0     # 最大ボーナス
        penalty_scale: 5.0  # ペナルティスケール
        weight: 5.0         # 重症度重み
      moderate:
        target: 13
        max_bonus: 20.0
        penalty_scale: 2.0
        weight: 2.0
      mild:
        target: 13
        max_bonus: 10.0
        penalty_scale: 0.5
        weight: 1.0
    
    # 離散報酬モード用パラメータ
    discrete_params:
      weights:
        response_time: 2.0
        severity_bonus: 3.0
        coverage_preservation: 0.5
      penalties:
        over_6min: -10.0
        over_13min: -15.0
        per_minute_over: -2.0
    
    # カバレッジ影響（共通）
    coverage_impact_weight: 0.5
    
  # エピソードレベル：エピソード全体の評価（必要に応じて調整）
  episode:
    base_penalty_per_minute: -1.0
    achievement_bonuses:
      rate_6min: 50.0
      rate_13min: 30.0
      critical_6min_rate: 100.0
    failure_penalty_per_incident: -1.0
    
  # 従来の設定（後方互換性のため保持）
  legacy:
    # 各要素の重み
    weights:
      response_time: -1.0  # 応答時間ペナルティ
      severity_bonus: 2.0  # 傷病度ボーナス係数
      threshold_penalty: -10.0  # 閾値超過ペナルティ
      coverage_preservation: 0.5  # カバレッジ維持報酬
      
    # ペナルティ設定
    penalties:
      over_6min: -5.0  # 6分超過基本ペナルティ
      over_13min: -20.0  # 13分超過基本ペナルティ
      per_minute_over: -1.0  # 1分あたり追加ペナルティ

# ================================================================
# ネットワーク構造（調整頻度：低）
# ================================================================
network:
  # 状態エンコーダ
  state_encoder:
    ambulance_features: 8  # ← 変更箇所：救急車特徴量次元
    incident_features: 6  # ← 変更箇所：事案特徴量次元
    spatial_features: 16  # ← 変更箇所：空間特徴量次元
    temporal_features: 8  # ← 変更箇所：時間特徴量次元
    
  # Actor/Criticネットワーク
  actor:
    hidden_layers: [256, 128, 64]  # ← 変更箇所：隠れ層構造
    activation: "relu"  # "relu", "tanh", "gelu"
    dropout: 0.1  # ← 変更箇所：ドロップアウト率
    
  critic:
    hidden_layers: [256, 128]  # ← 変更箇所：隠れ層構造
    activation: "relu"
    dropout: 0.1

# ================================================================
# 学習管理（調整頻度：中）
# ================================================================
training:
  # チェックポイント
  checkpoint_interval: 100  # ← 変更箇所：保存間隔（エピソード）
  keep_last_n: 5  # 保持する最新モデル数
  
  # 早期終了
  early_stopping:
    enabled: true
    patience: 500  # ← 変更箇所：改善なしエピソード数
    min_delta: 0.001  # 最小改善幅
    
  # ログ設定
  logging:
    interval: 10  # ← 変更箇所：ログ出力間隔
    tensorboard: true
    wandb: true  # Weights & Biases使用時はtrue
    
# ================================================================
# 評価設定（調整頻度：中）
# ================================================================
evaluation:
  interval: 100  # ← 変更箇所：評価実行間隔（エピソード）
  n_eval_episodes: 10  # ← 変更箇所：評価エピソード数
  
  # 評価指標
  metrics:
    - "mean_response_time"
    - "response_time_by_severity"
    - "6min_achievement_rate"
    - "13min_achievement_rate"
    - "critical_6min_rate"  # 重症・重篤・死亡の6分達成率
    
  # ベースライン比較
  compare_baselines:
    - "closest"  # 直近隊運用
    - "severity_based"  # ルールベース


# ================================================================
# ハイブリッドモード設定（重症系直近隊、軽症系PPO）
# ================================================================
hybrid_mode:
  enabled: false  # デフォルトは無効（train_ppo.pyで上書き可能）
  
  # 傷病度分類
  severity_classification:
    severe_conditions: ["重症", "重篤", "死亡"]  # 直近隊運用
    mild_conditions: ["軽症", "中等症"]           # PPO学習対象
  
  # 報酬設計
  reward_weights:
    response_time: 0.4      # A: 軽症系RT最小化（40%）
    coverage: 0.5           # B: カバレッジ維持（50%）
    workload_balance: 0.1   # C: 稼働バランス（10%）
  
  # 時間閾値（分）
  time_thresholds:
    good: 13       # 良好レベル
    warning: 20    # 警告レベル（超過で重いペナルティ）
  
  # ペナルティ設定
  penalties:
    over_warning: -50.0  # 20分超過時のペナルティ
    per_minute_over: -2.0  # 追加の分あたりペナルティ
  
  # カバレッジ評価
  coverage_evaluation:
    high_risk_weight: 0.7  # 高リスク地域の重み
    normal_weight: 0.3     # 通常地域の重み
    min_acceptable: 0.6    # 最低許容カバレッジ率